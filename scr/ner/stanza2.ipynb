{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/componavt/topkar-space/blob/main/stanza2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "42yem35y-bRm"
      },
      "outputs": [],
      "source": [
        "!pip install stanza\n",
        "!pip install pandas\n",
        "!pip install nltk\n",
        "!pip install pymorphy3\n",
        "!pip install re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EeA5TjMU-oU0"
      },
      "outputs": [],
      "source": [
        "import stanza\n",
        "\n",
        "# Загружаем модель для русского языка\n",
        "stanza.download('ru')  # Загрузите модель, если она еще не загружена\n",
        "nlp = stanza.Pipeline('ru')\n",
        "\n",
        "# Пример текста на русском языке\n",
        "text = \"Дом в дер. Койвусельга, фам. Михайлов. =\"\n",
        "\n",
        "# Обрабатываем текст\n",
        "doc = nlp(text)\n",
        "\n",
        "# Извлекаем именованные сущности\n",
        "for sentence in doc.sentences:\n",
        "    for entity in sentence.ents:\n",
        "        print(f'Текст: {entity.text}, Тип: {entity.type}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M98qUt_S_Imy"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "csv_files = [\n",
        "#    \"https://raw.githubusercontent.com/componavt/topkar-space/main/data/sample10.csv\",\n",
        "    \"https://raw.githubusercontent.com/componavt/topkar-space/main/data/sample100.csv\",\n",
        "]\n",
        "\n",
        "df = pd.concat([pd.read_csv(url, sep = ';') for url in csv_files], ignore_index=True)\n",
        "df = df.reset_index()  # make sure indexes pair with number of rows\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z7nWiBY6_kwY"
      },
      "outputs": [],
      "source": [
        "import stanza\n",
        "\n",
        "text = \"Яблоко рассматривает возможность покупки стартапа в Великобритании за 1 миллиард долларов. Владимир Путин является президентом России. А ещё паста и Кижи\"\n",
        "\n",
        "doc = nlp(text)\n",
        "\n",
        "for sentence in doc.sentences:\n",
        "    for entity in sentence.ents:\n",
        "        print(f'Текст: {entity.text}, Тип: {entity.type}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NRrxRHIq6ZGR"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "NER with lemmatizator\n",
        "\"\"\"\n",
        "from ast import Import\n",
        "from asyncio.windows_events import NULL\n",
        "import nltk\n",
        "import re\n",
        "import pymorphy3\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "lines = df['Text'].tolist()\n",
        "\n",
        "morph = pymorphy3.MorphAnalyzer()\n",
        "\n",
        "print()\n",
        "print(\"Number of toponyms:\", len(lines))\n",
        "toponyms = []\n",
        "df['Text'] = df['Text'].replace({float('nan'): \"\"})\n",
        "num=0\n",
        "output_csv = 'output.csv'\n",
        "with open(output_csv, 'w', encoding='utf-8') as outfile:\n",
        "  outfile.writelines(f\"sentence_id; toponyms_list \\n\")\n",
        "  if sentence != NULL:\n",
        "    for sentence in lines:\n",
        "        tex = nlp(sentence)\n",
        "        print(f'\\nSentence {num}: {sentence}')\n",
        "        num+=1\n",
        "        for entity in tex.ents:\n",
        "          if entity.type == \"LOC\":\n",
        "            print(f'Location: {entity.text}')\n",
        "            parsed = morph.parse(entity.text)[0]\n",
        "            normalized_word = parsed.normal_form\n",
        "            # print(f'Location: {entity.text}, type: {entity.type}')\n",
        "            toponyms.append(normalized_word)\n",
        "            print(f'Location: {normalized_word}')\n",
        "        strk = (f\"{num}; {toponyms} \\n\")\n",
        "        strk = strk.replace('[', '').replace(']', '').replace('\\'', '').replace('\\\"', '')\n",
        "        outfile.writelines(strk)\n",
        "        toponyms = []\n",
        "\n",
        "#print(\"Число найденных топонимов\", len(toponyms))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VLCGdtp11Sy5"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "NER without lemmatizator\n",
        "\"\"\"\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "lines = df['Text'].tolist()\n",
        "print()\n",
        "print(\"Number of toponyms:\", len(lines))\n",
        "toponyms = []\n",
        "df['Text'] = df['Text'].replace({float('nan'): \"\"})\n",
        "num=0\n",
        "output_csv = 'output.csv'\n",
        "with open(output_csv, 'w', encoding='utf-8') as outfile:\n",
        "  outfile.writelines(f\"sentence_id; toponyms_list \\n\")\n",
        "  for sentence in lines:\n",
        "      tex = nlp(sentence)\n",
        "      print(f'\\nSentence {num}: {sentence}')\n",
        "      num+=1\n",
        "      for entity in tex.ents:\n",
        "        if entity.type == \"LOC\":\n",
        "          print(f'Location: {entity.text}')\n",
        "          # print(f'Location: {entity.text}, type: {entity.type}')\n",
        "          toponyms.append(entity.text)\n",
        "      strk = (f\"{num}; {toponyms} \\n\")\n",
        "      strk = strk.replace('[', '').replace(']', '').replace('\\'', '').replace('\\\"', '')\n",
        "      outfile.writelines(strk)\n",
        "      toponyms = []\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wAD5GlBu1Sy6"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Comparison Analysis between output.csv and reference_table_sample100.csv\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "def clean_toponym_list(toponym_str):\n",
        "    \"\"\"Clean and normalize toponym lists for comparison\"\"\"\n",
        "    if pd.isna(toponym_str) or toponym_str == '':\n",
        "        return []\n",
        "\n",
        "    # Split by comma and clean each toponym\n",
        "    toponyms = [t.strip() for t in str(toponym_str).split(',')]\n",
        "    # Remove empty strings\n",
        "    toponyms = [t for t in toponyms if t]\n",
        "    return toponyms\n",
        "\n",
        "def compare_toponym_lists(list1, list2):\n",
        "    \"\"\"Compare two lists of toponyms and return differences\"\"\"\n",
        "    set1 = set(list1)\n",
        "    set2 = set(list2)\n",
        "\n",
        "    only_in_1 = set1 - set2\n",
        "    only_in_2 = set2 - set1\n",
        "    common = set1 & set2\n",
        "\n",
        "    return {\n",
        "        'only_in_output': list(only_in_1),\n",
        "        'only_in_reference': list(only_in_2),\n",
        "        'common': list(common),\n",
        "        'output_count': len(list1),\n",
        "        'reference_count': len(list2),\n",
        "        'common_count': len(common)\n",
        "    }\n",
        "\n",
        "\n",
        "# Read the datasets\n",
        "print(\"Loading datasets...\")\n",
        "\n",
        "# Read output.csv\n",
        "output_df = pd.read_csv('output.csv', sep=';')\n",
        "print(f\"Output dataset: {len(output_df)} rows\")\n",
        "\n",
        "# Read reference table\n",
        "reference_df = pd.read_csv('reference_table_sample100.csv')\n",
        "print(f\"Reference dataset: {len(reference_df)} rows\")\n",
        "\n",
        "# Clean column names\n",
        "output_df.columns = output_df.columns.str.strip()\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"DATASET STRUCTURE COMPARISON\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(f\"\\nOutput.csv columns: {list(output_df.columns)}\")\n",
        "print(f\"Reference table columns: {list(reference_df.columns)}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"TOPONYM LIST COMPARISON\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Compare toponym lists\n",
        "comparison_results = []\n",
        "\n",
        "for idx in range(len(output_df)):\n",
        "    output_row = output_df.iloc[idx]\n",
        "    reference_row = reference_df.iloc[idx] if idx < len(reference_df) else None\n",
        "\n",
        "    # Get toponyms from output.csv\n",
        "    output_toponyms = clean_toponym_list(output_row['toponyms_list'])\n",
        "\n",
        "    # Get toponyms from reference table\n",
        "    if reference_row is not None:\n",
        "        reference_toponyms = clean_toponym_list(reference_row['Toponim_list'])\n",
        "    else:\n",
        "        reference_toponyms = []\n",
        "\n",
        "    # Compare the lists\n",
        "    comparison = compare_toponym_lists(output_toponyms, reference_toponyms)\n",
        "    comparison['sentence_id'] = output_row['sentence_id']\n",
        "    comparison['output_text'] = output_row['toponyms_list']\n",
        "    comparison['reference_text'] = reference_row['Toponim_list'] if reference_row is not None else ''\n",
        "\n",
        "    comparison_results.append(comparison)\n",
        "\n",
        "# Create summary statistics\n",
        "total_only_in_output = sum(len(r['only_in_output']) for r in comparison_results)\n",
        "total_only_in_reference = sum(len(r['only_in_reference']) for r in comparison_results)\n",
        "total_common = sum(r['common_count'] for r in comparison_results)\n",
        "total_output = sum(r['output_count'] for r in comparison_results)\n",
        "total_reference = sum(r['reference_count'] for r in comparison_results)\n",
        "\n",
        "print(f\"\\nSUMMARY STATISTICS:\")\n",
        "print(f\"Total toponyms in output.csv: {total_output}\")\n",
        "print(f\"Total toponyms in reference table: {total_reference}\")\n",
        "print(f\"Common toponyms: {total_common}\")\n",
        "print(f\"Only in output.csv: {total_only_in_output}\")\n",
        "print(f\"Only in reference table: {total_only_in_reference}\")\n",
        "\n",
        "# Calculate accuracy metrics\n",
        "if total_reference > 0:\n",
        "    precision = total_common / total_output if total_output > 0 else 0\n",
        "    recall = total_common / total_reference\n",
        "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "\n",
        "    print(f\"\\nACCURACY METRICS:\")\n",
        "    print(f\"Precision: {precision:.3f} ({precision*100:.1f}%)\")\n",
        "    print(f\"Recall: {recall:.3f} ({recall*100:.1f}%)\")\n",
        "    print(f\"F1-Score: {f1_score:.3f} ({f1_score*100:.1f}%)\")\n",
        "\n",
        "# Find rows with significant differences\n",
        "print(f\"\\n\" + \"=\"*80)\n",
        "print(\"ROWS WITH SIGNIFICANT DIFFERENCES\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "significant_diffs = []\n",
        "for r in comparison_results:\n",
        "    if len(r['only_in_output']) > 0 or len(r['only_in_reference']) > 0:\n",
        "        significant_diffs.append(r)\n",
        "\n",
        "print(f\"Found {len(significant_diffs)} rows with differences\")\n",
        "\n",
        "# Show first 10 significant differences\n",
        "for i, diff in enumerate(significant_diffs[:10]):\n",
        "    print(f\"\\nRow {diff['sentence_id']}:\")\n",
        "    print(f\"  Output: '{diff['output_text']}'\")\n",
        "    print(f\"  Reference: '{diff['reference_text']}'\")\n",
        "    if diff['only_in_output']:\n",
        "        print(f\"  Only in output: {diff['only_in_output']}\")\n",
        "    if diff['only_in_reference']:\n",
        "        print(f\"  Only in reference: {diff['only_in_reference']}\")\n",
        "    if diff['common']:\n",
        "        print(f\"  Common: {diff['common']}\")\n",
        "\n",
        "# Analyze most common differences\n",
        "print(f\"\\n\" + \"=\"*80)\n",
        "print(\"MOST COMMON DIFFERENCES\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "all_only_in_output = []\n",
        "all_only_in_reference = []\n",
        "\n",
        "for r in comparison_results:\n",
        "    all_only_in_output.extend(r['only_in_output'])\n",
        "    all_only_in_reference.extend(r['only_in_reference'])\n",
        "\n",
        "print(f\"\\nMost frequent toponyms only in output.csv:\")\n",
        "output_counter = Counter(all_only_in_output)\n",
        "for toponym, count in output_counter.most_common(10):\n",
        "    print(f\"  '{toponym}': {count} times\")\n",
        "\n",
        "print(f\"\\nMost frequent toponyms only in reference table:\")\n",
        "reference_counter = Counter(all_only_in_reference)\n",
        "for toponym, count in reference_counter.most_common(10):\n",
        "    print(f\"  '{toponym}': {count} times\")\n",
        "\n",
        "# Save detailed comparison to CSV\n",
        "comparison_df = pd.DataFrame(comparison_results)\n",
        "comparison_df.to_csv('detailed_comparison.csv', index=False)\n",
        "print(f\"\\nDetailed comparison saved to 'detailed_comparison.csv'\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
