{
 "cells": [
 {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/componavt/topkar-space/blob/main/src/ner/distance_based_abbr_optimized.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
 },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –±–∏–±–ª–∏–æ—Ç–µ–∫\n",
    "!pip install stanza\n",
    "!pip install pandas\n",
    "!pip install nltk\n",
    "!pip install pymorphy3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import pandas as pd\n",
    "import re\n",
    "import csv\n",
    "from ast import Import\n",
    "import nltk\n",
    "import re\n",
    "import time\n",
    "from datetime import timedelta\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import stanza\n",
    "\n",
    "# –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ Stanza –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞\n",
    "stanza.download('ru', verbose=False)\n",
    "nlp = stanza.Pipeline('ru', processors='tokenize,ner', verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ —Å–æ–∫—Ä–∞—â–µ–Ω–∏–π –∏–∑ CSV —Ñ–∞–π–ª–∞\n",
    "def load_abbreviations(csv_filename):\n",
    "    \"\"\"\n",
    "    –ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å–æ–∫—Ä–∞—â–µ–Ω–∏—è –∏–∑ CSV —Ñ–∞–π–ª–∞\n",
    "    \"\"\n",
    "    print(f\"‚è≥ –ó–∞–≥—Ä—É–∑–∫–∞ —Å–æ–∫—Ä–∞—â–µ–Ω–∏–π –∏–∑ {csv_filename}...\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    abbreviations = {}\n",
    "    try:\n",
    "        with open(csv_filename, 'r', encoding='utf-8') as csvfile:\n",
    "            reader = csv.DictReader(csvfile, delimiter=';')\n",
    "            for row in reader:\n",
    "                full_name = row['–û–±–æ–∑–Ω–∞—á–µ–Ω–∏–µ']\n",
    "                short_forms_str = row['–°–æ–∫—Ä–∞—â–µ–Ω–∏—è']\n",
    "\n",
    "                if short_forms_str:\n",
    "                    # –†–∞–∑–¥–µ–ª—è–µ–º –ø–æ –∑–∞–ø—è—Ç–æ–π –∏ —É–±–∏—Ä–∞–µ–º –ø—Ä–æ–±–µ–ª—ã\n",
    "                    short_forms = [s.strip() for s in short_forms_str.split(',')]\n",
    "\n",
    "                    for short in short_forms:\n",
    "                        if short:\n",
    "                            abbreviations[short] = full_name\n",
    "                            print(f\"–î–æ–±–∞–≤–ª–µ–Ω–æ —Å–æ–∫—Ä–∞—â–µ–Ω–∏–µ: '{short}' -> {full_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading abbreviations: {e}\")\n",
    "        # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π —Å–ø–æ—Å–æ–± —á—Ç–µ–Ω–∏—è –µ—Å–ª–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ —Ñ–∞–π–ª–∞ –æ—Ç–ª–∏—á–∞–µ—Ç—Å—è\n",
    "        try:\n",
    "            with open(csv_filename, 'r', encoding='utf-8') as f:\n",
    "                lines = f.readlines()\n",
    "                for line in lines:\n",
    "                    if ';' in line:\n",
    "                        parts = line.strip().split(';')\n",
    "                        if len(parts) >= 2:\n",
    "                            full_name = parts[0].strip()\n",
    "                            short_forms_str = parts[1].strip()\n",
    "                            short_forms = [s.strip() for s in short_forms_str.split(',')]\n",
    "                            for short in short_forms:\n",
    "                                if short:\n",
    "                                    abbreviations[short] = full_name\n",
    "        except Exception as e2:\n",
    "            print(f\"Alternative loading also failed: {e2}\")\n",
    "\n",
    "    load_time = time.time() - start_time\n",
    "    print(f\" –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(abbreviations)} —Å–æ–∫—Ä–∞—â–µ–Ω–∏–π –∑–∞ {load_time:.2f} —Å–µ–∫—É–Ω–¥\")\n",
    "    return abbreviations"
   ]
 },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –§—É–Ω–∫—Ü–∏—è –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ —Ä–µ–≥—É–ª—è—Ä–Ω—ã—Ö –≤—ã—Ä–∞–∂–µ–Ω–∏–π\n",
    "def create_abbreviation_patterns(abbreviations_dict):\n",
    "    \"\"\"\n",
    "    –°–æ–∑–¥–∞–µ—Ç –ø–∞—Ç—Ç–µ—Ä–Ω—ã —Ä–µ–≥—É–ª—è—Ä–Ω—ã—Ö –≤—ã—Ä–∞–∂–µ–Ω–∏–π –¥–ª—è –≤—Å–µ—Ö —Å–æ–∫—Ä–∞—â–µ–Ω–∏–π\n",
    "    \"\"\"\n",
    "    print(f\"üîß –°–æ–∑–¥–∞–Ω–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –¥–ª—è {len(abbreviations_dict)} —Å–æ–∫—Ä–∞—â–µ–Ω–∏–π...\")\n",
    "    patterns = {}\n",
    "\n",
    "    for short_form, full_name in abbreviations_dict.items():\n",
    "        if '.' in short_form:\n",
    "            # –î–ª—è —Å–æ–∫—Ä–∞—â–µ–Ω–∏–π —Å —Ç–æ—á–∫–æ–π: –∏—â–µ–º —Å —Ç–æ—á–∫–æ–π, –ø–æ—Å–ª–µ –∫–æ—Ç–æ—Ä–æ–π –º–æ–∂–µ—Ç –±—ã—Ç—å –ø—Ä–æ–±–µ–ª –∏–ª–∏ –∫–æ–Ω–µ—Ü —Å—Ç—Ä–æ–∫–∏\n",
    "            pattern = r'\\b' + re.escape(short_form) + r'(?=\\s|$|\\.|,|;)'\n",
    "        else:\n",
    "            # –î–ª—è —Å–æ–∫—Ä–∞—â–µ–Ω–∏–π –±–µ–∑ —Ç–æ—á–∫–∏: –æ–±—ã—á–Ω—ã–π –ø–æ–∏—Å–∫ —Å –≥—Ä–∞–Ω–∏—Ü–∞–º–∏ —Å–ª–æ–≤\n",
    "            pattern = r'\\b' + re.escape(short_form) + r'\\b'\n",
    "        patterns[short_form] = (pattern, full_name)\n",
    "\n",
    "    print(f\" –ü–∞—Ç—Ç–µ—Ä–Ω—ã —Å–æ–∑–¥–∞–Ω—ã –¥–ª—è {len(patterns)} —Å–æ–∫—Ä–∞—â–µ–Ω–∏–π\")\n",
    "    return patterns"
   ]
  },
 {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–æ–∏—Å–∫–∞ —Å–æ–∫—Ä–∞—â–µ–Ω–∏–π –≤ –æ–∫—Ä–µ—Å—Ç–Ω–æ—Å—Ç–∏ —Ç–æ–ø–æ–Ω–∏–º–∞\n",
    "def find_nearby_abbreviations(text, toponym, patterns, distance_words=2):\n",
    "    \"\"\"\n",
    "    –ò—â–µ—Ç —Å–æ–∫—Ä–∞—â–µ–Ω–∏—è –≤ –æ–∫—Ä–µ—Å—Ç–Ω–æ—Å—Ç–∏ —Ç–æ–ø–æ–Ω–∏–º–∞ –∏—Å–ø–æ–ª—å–∑—É—è –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ —Å–æ–∑–¥–∞–Ω–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã\n",
    "    \"\"\"\n",
    "    search_start_time = time.time()\n",
    "    nearby_abbreviations = []\n",
    "\n",
    "    # –ù–∞—Ö–æ–¥–∏–º –≤—Å–µ –≤—Ö–æ–∂–¥–µ–Ω–∏—è —Ç–æ–ø–æ–Ω–∏–º–∞ –≤ —Ç–µ–∫—Å—Ç–µ\n",
    "    toponym_search_start = time.time()\n",
    "    toponym_pattern = re.escape(toponym)\n",
    "    toponym_matches = list(re.finditer(toponym_pattern, text))\n",
    "    toponym_search_time = time.time() - toponym_search_start\n",
    "    print(f\"    –ü–æ–∏—Å–∫ —Ç–æ–ø–æ–Ω–∏–º–∞ '{toponym}': {toponym_search_time:.4f} —Å–µ–∫, –Ω–∞–π–¥–µ–Ω–æ {len(toponym_matches)} –≤—Ö–æ–∂–¥–µ–Ω–∏–π\")\n",
    "\n",
    "    for toponym_match in toponym_matches:\n",
    "        toponym_start = toponym_match.start()\n",
    "        toponym_end = toponym_match.end()\n",
    "\n",
    "        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –æ–∫—Ä–µ—Å—Ç–Ω–æ—Å—Ç—å –≤–æ–∫—Ä—É–≥ —Ç–æ–ø–æ–Ω–∏–º–∞ (–≤ —Å–∏–º–≤–æ–ª–∞—Ö)\n",
    "        # –ü—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω–æ: —Å—á–∏—Ç–∞–µ–º —á—Ç–æ –≤ —Å—Ä–µ–¥–Ω–µ–º —Å–ª–æ–≤–æ = 6 —Å–∏–º–≤–æ–ª–æ–≤ + –ø—Ä–æ–±–µ–ª\n",
    "        approx_distance_chars = distance_words * 7\n",
    "\n",
    "        search_start = max(0, toponym_start - approx_distance_chars)\n",
    "        search_end = min(len(text), toponym_end + approx_distance_chars)\n",
    "\n",
    "        search_area = text[search_start:search_end]\n",
    "\n",
    "        # –ò—â–µ–º –≤—Å–µ —Å–æ–∫—Ä–∞—â–µ–Ω–∏—è –≤ —ç—Ç–æ–π –æ–±–ª–∞—Å—Ç–∏\n",
    "        abbreviation_search_start = time.time()\n",
    "        for short_form, (pattern, full_name) in patterns.items():\n",
    "            abbreviation_matches = list(re.finditer(pattern, search_area))\n",
    "\n",
    "            for abbr_match in abbreviation_matches:\n",
    "                # –í—ã—á–∏—Å–ª—è–µ–º –∞–±—Å–æ–ª—é—Ç–Ω—É—é –ø–æ–∑–∏—Ü–∏—é —Å–æ–∫—Ä–∞—â–µ–Ω–∏—è –≤ —Ç–µ–∫—Å—Ç–µ\n",
    "                abbr_absolute_pos = search_start + abbr_match.start()\n",
    "\n",
    "                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ —Å–æ–∫—Ä–∞—â–µ–Ω–∏–µ –Ω–µ —è–≤–ª—è–µ—Ç—Å—è —á–∞—Å—Ç—å—é —Å–∞–º–æ–≥–æ —Ç–æ–ø–æ–Ω–∏–º–∞\n",
    "                if not (toponym_start <= abbr_absolute_pos <= toponym_end):\n",
    "                    abbr_info = {\n",
    "                        'abbreviation': abbr_match.group(),\n",
    "                        'full_name': full_name,\n",
    "                        'position': abbr_absolute_pos,\n",
    "                        'context': search_area\n",
    "                    }\n",
    "\n",
    "                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –¥—É–±–ª–∏–∫–∞—Ç—ã\n",
    "                    is_duplicate = False\n",
    "                    for existing in nearby_abbreviations:\n",
    "                        if (existing['abbreviation'] == abbr_info['abbreviation'] and\n",
    "                            existing['full_name'] == abbr_info['full_name'] and\n",
    "                            abs(existing['position'] - abbr_info['position']) < 10):\n",
    "                            is_duplicate = True\n",
    "                            break\n",
    "\n",
    "                    if not is_duplicate:\n",
    "                        nearby_abbreviations.append(abbr_info)\n",
    "\n",
    "        abbreviation_search_time = time.time() - abbreviation_search_start\n",
    "        print(f\"    –ü–æ–∏—Å–∫ —Å–æ–∫—Ä–∞—â–µ–Ω–∏–π –≤–æ–∫—Ä—É–≥ '{toponym}': {abbreviation_search_time:.4f} —Å–µ–∫\")\n",
    "\n",
    "    total_search_time = time.time() - search_start_time\n",
    "    if nearby_abbreviations:\n",
    "        print(f\"    –ù–∞–π–¥–µ–Ω–æ {len(nearby_abbreviations)} —Å–æ–∫—Ä–∞—â–µ–Ω–∏–π –∑–∞ {total_search_time:.4f} —Å–µ–∫\")\n",
    "    else:\n",
    "        print(f\"    –°–æ–∫—Ä–∞—â–µ–Ω–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –∑–∞ {total_search_time:.4f} —Å–µ–∫\")\n",
    "\n",
    "    return nearby_abbreviations"
   ]
  },
 {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(max_lines):\n",
    "    \"\"\"\n",
    "    –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Å—Ç—Ä–æ–∫\n",
    "\n",
    "    Args:\n",
    "        max_lines (int): –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–æ–∫ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏. –ï—Å–ª–∏ None - –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –≤—Å–µ.\n",
    "    \"\"\"\n",
    "    total_start_time = time.time()\n",
    "\n",
    "    # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å–æ–∫—Ä–∞—â–µ–Ω–∏—è\n",
    "    abbreviations_dict = load_abbreviations('abbreviations.csv')\n",
    "\n",
    "    # –°–æ–∑–¥–∞–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã —Ä–µ–≥—É–ª—è—Ä–Ω—ã—Ö –≤—ã—Ä–∞–∂–µ–Ω–∏–π –æ–¥–∏–Ω —Ä–∞–∑ –¥–ª—è –≤—Å–µ—Ö –≤—ã–∑–æ–≤–æ–≤\n",
    "    patterns_creation_start = time.time()\n",
    "    patterns = create_abbreviation_patterns(abbreviations_dict)\n",
    "    patterns_creation_time = time.time() - patterns_creation_start\n",
    "    print(f\"‚è± –û–±—â–µ–µ –≤—Ä–µ–º—è —Å–æ–∑–¥–∞–Ω–∏—è –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤: {patterns_creation_time:.4f} —Å–µ–∫\")\n",
    "\n",
    "    lines = df['main_text'].tolist()\n",
    "    print(f\"\\n –í—Å–µ–≥–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –≤ –¥–∞–Ω–Ω—ã—Ö: {len(lines)}\")\n",
    "\n",
    "    # –ü—Ä–∏–º–µ–Ω—è–µ–º –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É —Å—Ç—Ä–æ–∫\n",
    "    if max_lines is not None:\n",
    "        lines = lines[:max_lines]\n",
    "        print(f\" –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã—Ö {max_lines} —Å—Ç—Ä–æ–∫\")\n",
    "\n",
    "    df['main_text'] = df['main_text'].replace({float('nan'): \"\"})\n",
    "\n",
    "    num = 0\n",
    "    output_csv = 'output.csv'\n",
    "\n",
    "    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞\n",
    "    total_toponyms = 0\n",
    "    total_abbreviations = 0\n",
    "    processed_sentences = 0\n",
    "\n",
    "    # –í—Ä–µ–º—è –¥–ª—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏\n",
    "    total_toponym_time = 0\n",
    "    total_abbreviation_time = 0\n",
    "    total_nlp_time = 0\n",
    "\n",
    "    print(f\"\\n –ù–∞—á–∞–ª–æ –æ–±—Ä–∞–±–æ—Ç–∫–∏ {len(lines)} –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π...\")\n",
    "    processing_start_time = time.time()\n",
    "\n",
    "    with open(output_csv, 'w', encoding='utf-8') as outfile:\n",
    "        outfile.writelines(f\"sentence_id; toponyms_list; nearby_abbreviations \\n\")\n",
    "\n",
    "        # –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø: –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –±–∞—Ç—á–∞–º–∏ –¥–ª—è –ª—É—á—à–µ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏\n",
    "        batch_size = 100  # –ú–æ–∂–Ω–æ –Ω–∞—Å—Ç—Ä–æ–∏—Ç—å –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –æ–±—ä–µ–º–∞ –ø–∞–º—è—Ç–∏\n",
    "\n",
    "        for i in range(0, len(lines), batch_size):\n",
    "            batch_lines = lines[i:i + batch_size]\n",
    "            batch_indices = list(range(i, min(i + batch_size, len(lines))))\n",
    "\n",
    "            print(f\"\\n--- –û–±—Ä–∞–±–æ—Ç–∫–∞ –±–∞—Ç—á–∞ {i//batch_size + 1}/{(len(lines)-1)//batch_size + 1} ---\")\n",
    "\n",
    "            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –±–∞—Ç—á –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π\n",
    "            nlp_start_time = time.time()\n",
    "            \n",
    "            # –§–∏–ª—å—Ç—Ä—É–µ–º –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏ –ø–µ—Ä–µ–¥ –ø–µ—Ä–µ–¥–∞—á–µ–π –≤ bulk_process\n",
    "            non_empty_lines = [(idx, text) for idx, text in enumerate(batch_lines) if pd.notna(text) and text != \"\"]\n",
    "            if non_empty_lines:\n",
    "                # –†–∞–∑–¥–µ–ª—è–µ–º –∏–Ω–¥–µ–∫—Å—ã –∏ —Ç–µ–∫—Å—Ç—ã\n",
    "                non_empty_indices, non_empty_texts = zip(*non_empty_lines)\n",
    "                \n",
    "                # –í—ã–ø–æ–ª–Ω—è–µ–º bulk_process —Ç–æ–ª—å–∫–æ –¥–ª—è –Ω–µ–ø—É—Å—Ç—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤\n",
    "                batch_docs = nlp.bulk_process(non_empty_texts)\n",
    "                \n",
    "                # –í–æ—Å—Å–æ–∑–¥–∞–µ–º –ø–æ–ª–Ω—ã–π —Å–ø–∏—Å–æ–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤, –≤–∫–ª—é—á–∞—è None –¥–ª—è –ø—É—Å—Ç—ã—Ö —Å—Ç—Ä–æ–∫\n",
    "                batch_docs_full = [None] * len(batch_lines)\n",
    "                for idx, doc in zip(non_empty_indices, batch_docs):\n",
    "                    batch_docs_full[idx] = doc\n",
    "            else:\n",
    "                batch_docs_full = [None] * len(batch_lines)\n",
    "                \n",
    "            batch_docs = batch_docs_full\n",
    "            batch_nlp_time = time.time() - nlp_start_time\n",
    "            total_nlp_time += batch_nlp_time\n",
    "            print(f\"‚è± –í—Ä–µ–º—è NLP –æ–±—Ä–∞–±–æ—Ç–∫–∏ –±–∞—Ç—á–∞: {batch_nlp_time:.4f} —Å–µ–∫\")\n",
    "\n",
    "            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥—ã–π –¥–æ–∫—É–º–µ–Ω—Ç –≤ –±–∞—Ç—á–µ\n",
    "            for j, (doc, sentence_idx) in enumerate(zip(batch_docs, batch_indices)):\n",
    "                sentence = batch_lines[j]\n",
    "\n",
    "                if doc is None:\n",
    "                    # –ü—É—Å—Ç–∞—è —Å—Ç—Ä–æ–∫–∞\n",
    "                    strk = f\"{sentence_idx}; ; \\n\"\n",
    "                    outfile.writelines(strk)\n",
    "                    num += 1\n",
    "                    continue\n",
    "\n",
    "                sentence_start_time = time.time()\n",
    "\n",
    "                print(f'\\n--- –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ {sentence_idx} ---')\n",
    "                print(f'–¢–µ–∫—Å—Ç: {sentence[:100]}...' if len(sentence) > 100 else f'–¢–µ–∫—Å—Ç: {sentence}')\n",
    "\n",
    "                sentence_toponyms = []\n",
    "                all_nearby_abbreviations = []\n",
    "\n",
    "                # –ü–æ–∏—Å–∫ —Ç–æ–ø–æ–Ω–∏–º–æ–≤ –≤ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω–æ–º –¥–æ–∫—É–º–µ–Ω—Ç–µ\n",
    "                toponym_search_start = time.time()\n",
    "                for entity in doc.ents:\n",
    "                    if entity.type == \"LOC\":\n",
    "                        entity_find_time = time.time() - toponym_search_start\n",
    "                        print(f'üìç –¢–æ–ø–æ–Ω–∏–º: {entity.text} (–Ω–∞–π–¥–µ–Ω –∑–∞ {entity_find_time:.4f} —Å–µ–∫)')\n",
    "                        sentence_toponyms.append(entity.text)\n",
    "                        total_toponyms += 1\n",
    "\n",
    "                        # –û–±–Ω–æ–≤–ª—è–µ–º –≤—Ä–µ–º—è –¥–ª—è —Å–ª–µ–¥—É—é—â–µ–≥–æ —Ç–æ–ø–æ–Ω–∏–º–∞\n",
    "                        toponym_search_start = time.time()\n",
    "\n",
    "                        # –ò—â–µ–º —Å–æ–∫—Ä–∞—â–µ–Ω–∏—è –≤ –æ–∫—Ä–µ—Å—Ç–Ω–æ—Å—Ç–∏ —ç—Ç–æ–≥–æ —Ç–æ–ø–æ–Ω–∏–º–∞\n",
    "                        abbreviation_search_start = time.time()\n",
    "                        nearby_abbr = find_nearby_abbreviations_advanced(\n",
    "                            sentence,\n",
    "                            entity.text,\n",
    "                            patterns,\n",
    "                            distance_words=5\n",
    "                        )\n",
    "                        abbreviation_time = time.time() - abbreviation_search_start\n",
    "                        total_abbreviation_time += abbreviation_time\n",
    "\n",
    "                        print(f\"    –û–±—â–µ–µ –≤—Ä–µ–º—è –ø–æ–∏—Å–∫–∞ —Å–æ–∫—Ä–∞—â–µ–Ω–∏–π –¥–ª—è '{entity.text}': {abbreviation_time:.4f} —Å–µ–∫\")\n",
    "\n",
    "                        for abbr_info in nearby_abbr:\n",
    "                            match_str = f\"{abbr_info['abbreviation']}\"\n",
    "                            if match_str not in all_nearby_abbreviations:\n",
    "                                all_nearby_abbreviations.append(match_str)\n",
    "                                total_abbreviations += 1\n",
    "                                print(f\"    –ù–∞–π–¥–µ–Ω–æ —Å–æ–∫—Ä–∞—â–µ–Ω–∏–µ: '{abbr_info['abbreviation']}' -> '{abbr_info['full_name']}'\")\n",
    "\n",
    "                # –ï—Å–ª–∏ –Ω–µ –±—ã–ª–æ –Ω–∞–π–¥–µ–Ω–æ —Ç–æ–ø–æ–Ω–∏–º–æ–≤, –≤—ã–≤–æ–¥–∏–º –≤—Ä–µ–º—è –ø–æ–∏—Å–∫–∞\n",
    "                if not sentence_toponyms:\n",
    "                    toponym_time = time.time() - toponym_search_start\n",
    "                    total_toponym_time += toponym_time\n",
    "                    print(f\" –í—Ä–µ–º—è –ø–æ–∏—Å–∫–∞ —Ç–æ–ø–æ–Ω–∏–º–æ–≤: {toponym_time:.4f} —Å–µ–∫ (–Ω–µ –Ω–∞–π–¥–µ–Ω–æ)\")\n",
    "\n",
    "                # –§–æ—Ä–º–∏—Ä—É–µ–º —Å—Ç—Ä–æ–∫—É –¥–ª—è –≤—ã–≤–æ–¥–∞\n",
    "                toponyms_str = ', '.join(sentence_toponyms)\n",
    "                abbreviations_str = ', '.join(all_nearby_abbreviations)\n",
    "\n",
    "                strk = f\"{sentence_idx}; {toponyms_str}; {abbreviations_str} \\n\"\n",
    "                outfile.writelines(strk)\n",
    "\n",
    "                sentence_time = time.time() - sentence_start_time\n",
    "                print(f\" –û–±—â–µ–µ –≤—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è: {sentence_time:.2f} —Å–µ–∫\")\n",
    "\n",
    "                processed_sentences += 1\n",
    "                num += 1\n",
    "\n",
    "            # –ü—Ä–æ–≥—Ä–µ—Å—Å –ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–≥–æ –±–∞—Ç—á–∞\n",
    "            elapsed = time.time() - processing_start_time\n",
    "            print(f\"\\n –ü—Ä–æ–≥—Ä–µ—Å—Å: –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ {processed_sentences}/{len(lines)} –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π\")\n",
    "            print(f\" –ü—Ä–æ—à–ª–æ –≤—Ä–µ–º–µ–Ω–∏: {timedelta(seconds=int(elapsed))}\")\n",
    "\n",
    "            # –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –ø–æ —ç—Ç–∞–ø–∞–º\n",
    "            if processed_sentences > 0:\n",
    "                avg_nlp_time = total_nlp_time / processed_sentences\n",
    "                if total_toponyms > 0:\n",
    "                    avg_abbr_time = total_abbreviation_time / total_toponyms\n",
    "                    print(f\" –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –ø–æ–∏—Å–∫–∞ —Å–æ–∫—Ä–∞—â–µ–Ω–∏–π –Ω–∞ —Ç–æ–ø–æ–Ω–∏–º: {avg_abbr_time:.4f} —Å–µ–∫\")\n",
    "\n",
    "                print(f\" –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è NLP: {avg_nlp_time:.4f} —Å–µ–∫\")\n",
    "\n",
    "                avg_time = elapsed / processed_sentences\n",
    "                remaining = (len(lines) - processed_sentences) * avg_time\n",
    "                print(f\" –û—Å—Ç–∞–≤—à–µ–µ—Å—è –≤—Ä–µ–º—è: ~{timedelta(seconds=int(remaining))}\")\n",
    "\n",
    "    total_time = time.time() - total_start_time\n",
    "\n",
    "    # –§–∏–Ω–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Å –¥–µ—Ç–∞–ª–∏–∑–∞—Ü–∏–µ–π –ø–æ –≤—Ä–µ–º–µ–Ω–∏\n",
    "    print(f\"\\n –û–ë–†–ê–ë–û–¢–ö–ê –ó–ê–í–ï–†–®–ï–ù–ê!\")\n",
    "    print(f\" –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:\")\n",
    "    print(f\"   - –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π: {processed_sentences}\")\n",
    "    print(f\"   - –ù–∞–π–¥–µ–Ω–æ —Ç–æ–ø–æ–Ω–∏–º–æ–≤: {total_toponyms}\")\n",
    "    print(f\"   - –ù–∞–π–¥–µ–Ω–æ —Å–æ–∫—Ä–∞—â–µ–Ω–∏–π: {total_abbreviations}\")\n",
    "    print(f\"   - –û–±—â–µ–µ –≤—Ä–µ–º—è: {timedelta(seconds=int(total_time))}\")\n",
    "\n",
    "    if processed_sentences > 0:\n",
    "        print(f\"\\n‚è± –î–µ—Ç–∞–ª–∏–∑–∞—Ü–∏—è –≤—Ä–µ–º–µ–Ω–∏:\")\n",
    "        print(f\"   - –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ: {total_time/processed_sentences:.2f} —Å–µ–∫\")\n",
    "        print(f\"   - –û–±—â–µ–µ –≤—Ä–µ–º—è NLP: {timedelta(seconds=int(total_nlp_time))}\")\n",
    "        print(f\"   - –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è NLP –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ: {total_nlp_time/processed_sentences:.4f} —Å–µ–∫\")\n",
    "        print(f\"   - –í—Ä–µ–º—è —Å–æ–∑–¥–∞–Ω–∏—è –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤: {patterns_creation_time:.4f} —Å–µ–∫\")\n",
    "\n",
    "        if total_toponyms > 0:\n",
    "            print(f\"   - –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –ø–æ–∏—Å–∫–∞ —Å–æ–∫—Ä–∞—â–µ–Ω–∏–π –Ω–∞ —Ç–æ–ø–æ–Ω–∏–º: {total_abbreviation_time/total_toponyms:.4f} —Å–µ–∫\")\n",
    "            print(f\"   - –°—Ä–µ–¥–Ω–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–ø–æ–Ω–∏–º–æ–≤ –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ: {total_toponyms/processed_sentences:.2f}\")\n",
    "            print(f\"   - –°—Ä–µ–¥–Ω–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–æ–∫—Ä–∞—â–µ–Ω–∏–π –Ω–∞ —Ç–æ–ø–æ–Ω–∏–º: {total_abbreviations/max(total_toponyms, 1):.2f}\")\n",
    "\n",
    "    print(f\"   - –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {output_csv}\")"
   ]
  },
 {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω–∞—è –≤–µ—Ä—Å–∏—è —Å –±–æ–ª–µ–µ —Ç–æ—á–Ω—ã–º –ø–æ–∏—Å–∫–æ–º –ø–æ —Å–ª–æ–≤–∞–º\n",
    "def find_nearby_abbreviations_advanced(text, toponym, patterns, distance_words=2):\n",
    "    \"\"\"\n",
    "    –£–ª—É—á—à–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è –ø–æ–∏—Å–∫–∞ —Å —É—á–µ—Ç–æ–º —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è –≤ —Å–ª–æ–≤–∞—Ö\n",
    "    \"\"\"\n",
    "    search_start_time = time.time()\n",
    "    nearby_abbreviations = []\n",
    "\n",
    "    # –¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –ø–æ–∑–∏—Ü–∏–π\n",
    "    tokenization_start = time.time()\n",
    "    words_with_positions = []\n",
    "    pattern = r'\\b\\w+(?:\\.\\w+)*\\b|[\\W\\S]'\n",
    "    for match in re.finditer(pattern, text):\n",
    "        word = match.group()\n",
    "        if word.strip():  # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º —á–∏—Å—Ç—ã–µ –ø—Ä–æ–±–µ–ª—ã\n",
    "            words_with_positions.append({\n",
    "                'word': word,\n",
    "                'start': match.start(),\n",
    "                'end': match.end()\n",
    "            })\n",
    "    tokenization_time = time.time() - tokenization_start\n",
    "    print(f\"    –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞: {tokenization_time:.4f} —Å–µ–∫\")\n",
    "\n",
    "    # –ù–∞—Ö–æ–¥–∏–º –ø–æ–∑–∏—Ü–∏–∏ —Ç–æ–ø–æ–Ω–∏–º–∞\n",
    "    toponym_search_start = time.time()\n",
    "    toponym_positions = []\n",
    "    for i, word_info in enumerate(words_with_positions):\n",
    "        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ—Ç–¥–µ–ª—å–Ω—ã–µ —Å–ª–æ–≤–∞ –∏ –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏ —Å–ª–æ–≤ –¥–ª—è —Ç–æ–ø–æ–Ω–∏–º–∞\n",
    "        if word_info['word'] in toponym:\n",
    "            # –ü—Ä–æ—Å—Ç–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ - –º–æ–∂–Ω–æ —É–ª—É—á—à–∏—Ç—å –¥–ª—è –º–Ω–æ–≥–æ—Å–ª–æ–≤–Ω—ã—Ö —Ç–æ–ø–æ–Ω–∏–º–æ–≤\n",
    "            toponym_positions.append(i)\n",
    "    toponym_search_time = time.time() - toponym_search_start\n",
    "    print(f\"    –ü–æ–∏—Å–∫ –ø–æ–∑–∏—Ü–∏–π —Ç–æ–ø–æ–Ω–∏–º–∞: {toponym_search_time:.4f} —Å–µ–∫, –Ω–∞–π–¥–µ–Ω–æ {len(toponym_positions)} –ø–æ–∑–∏—Ü–∏–π\")\n",
    "\n",
    "    # –î–ª—è –∫–∞–∂–¥–æ–π –ø–æ–∑–∏—Ü–∏–∏ —Ç–æ–ø–æ–Ω–∏–º–∞ –∏—â–µ–º —Å–æ—Å–µ–¥–Ω–∏–µ —Å–æ–∫—Ä–∞—â–µ–Ω–∏—è\n",
    "    abbreviation_search_start = time.time()\n",
    "    for pos in toponym_positions:\n",
    "        start_idx = max(0, pos - distance_words)\n",
    "        end_idx = min(len(words_with_positions), pos + distance_words + 1)\n",
    "\n",
    "        for i in range(start_idx, end_idx):\n",
    "            word_info = words_with_positions[i]\n",
    "            word = word_info['word']\n",
    "\n",
    "            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Å–ª–æ–≤–æ —Å–æ–∫—Ä–∞—â–µ–Ω–∏–µ–º\n",
    "            for short_form, (pattern, full_name) in patterns.items():\n",
    "                # –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º —Å –æ—á–∏—â–µ–Ω–Ω–æ–π –≤–µ—Ä—Å–∏–µ–π —Å–æ–∫—Ä–∞—â–µ–Ω–∏—è\n",
    "                clean_short = short_form.replace('.', '')\n",
    "                clean_word = word.replace('.', '')\n",
    "\n",
    "                if clean_word == clean_short:\n",
    "                    abbr_info = {\n",
    "                        'abbreviation': word,\n",
    "                        'full_name': full_name,\n",
    "                        'position': word_info['start']\n",
    "                    }\n",
    "\n",
    "                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –¥—É–±–ª–∏–∫–∞—Ç—ã\n",
    "                    is_duplicate = False\n",
    "                    for existing in nearby_abbreviations:\n",
    "                        if (existing['abbreviation'] == abbr_info['abbreviation'] and\n",
    "                            existing['full_name'] == abbr_info['full_name']):\n",
    "                            is_duplicate = True\n",
    "                            break\n",
    "\n",
    "                    if not is_duplicate:\n",
    "                        nearby_abbreviations.append(abbr_info)\n",
    "\n",
    "    abbreviation_search_time = time.time() - abbreviation_search_start\n",
    "    total_search_time = time.time() - search_start_time\n",
    "\n",
    "    print(f\"    –ü–æ–∏—Å–∫ —Å–æ–∫—Ä–∞—â–µ–Ω–∏–π: {abbreviation_search_time:.4f} —Å–µ–∫\")\n",
    "    if nearby_abbreviations:\n",
    "        print(f\"    –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –ø–æ–∏—Å–∫: –Ω–∞–π–¥–µ–Ω–æ {len(nearby_abbreviations)} —Å–æ–∫—Ä–∞—â–µ–Ω–∏–π –∑–∞ {total_search_time:.4f} —Å–µ–∫\")\n",
    "    else:\n",
    "        print(f\"    –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –ø–æ–∏—Å–∫: —Å–æ–∫—Ä–∞—â–µ–Ω–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –∑–∞ {total_search_time:.4f} —Å–µ–∫\")\n",
    "\n",
    "    return nearby_abbreviations"
   ]
  },
 {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö\n",
    "csv_files = [\n",
    "    \"https://raw.githubusercontent.com/componavt/topkar-space/main/data/reference_abr_sample100.csv\",\n",
    "]\n",
    "\n",
    "df = pd.concat([pd.read_csv(url, sep = ';') for url in csv_files], ignore_index=True)\n",
    "df = df.reset_index()  # make sure indexes pair with number of rows\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ó–∞–ø—É—Å–∫ –æ—Å–Ω–æ–≤–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–∏\n",
    "if __name__ == \"__main__\":\n",
    "    # –û–±—Ä–∞–±–æ—Ç–∞—Ç—å —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ 50 —Å—Ç—Ä–æ–∫ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è\n",
    "    #main(max_lines=10)\n",
    "\n",
    "    # –ò–ª–∏ –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å –≤—Å–µ —Å—Ç—Ä–æ–∫–∏:\n",
    "    main(None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}