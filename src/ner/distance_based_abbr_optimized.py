import nltk
from nltk.tokenize import sent_tokenize, word_tokenize
import pandas as pd
import re
import csv
from ast import Import
import nltk
import re
import time
from datetime import timedelta
from nltk.tokenize import sent_tokenize, word_tokenize

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ —Å–æ–∫—Ä–∞—â–µ–Ω–∏–π –∏–∑ CSV —Ñ–∞–π–ª–∞
def load_abbreviations(csv_filename):
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å–æ–∫—Ä–∞—â–µ–Ω–∏—è –∏–∑ CSV —Ñ–∞–π–ª–∞
    """
    print(f"‚è≥ –ó–∞–≥—Ä—É–∑–∫–∞ —Å–æ–∫—Ä–∞—â–µ–Ω–∏–π –∏–∑ {csv_filename}...")
    start_time = time.time()

    abbreviations = {}
    try:
        with open(csv_filename, 'r', encoding='utf-8') as csvfile:
            reader = csv.DictReader(csvfile, delimiter=';')
            for row in reader:
                full_name = row['–û–±–æ–∑–Ω–∞—á–µ–Ω–∏–µ']
                short_forms_str = row['–°–æ–∫—Ä–∞—â–µ–Ω–∏—è']

                if short_forms_str:
                    # –†–∞–∑–¥–µ–ª—è–µ–º –ø–æ –∑–∞–ø—è—Ç–æ–π –∏ —É–±–∏—Ä–∞–µ–º –ø—Ä–æ–±–µ–ª—ã
                    short_forms = [s.strip() for s in short_forms_str.split(',')]

                    for short in short_forms:
                        if short:
                            abbreviations[short] = full_name
                            print(f"–î–æ–±–∞–≤–ª–µ–Ω–æ —Å–æ–∫—Ä–∞—â–µ–Ω–∏–µ: '{short}' -> {full_name}")
    except Exception as e:
        print(f"Error loading abbreviations: {e}")
        # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π —Å–ø–æ—Å–æ–± —á—Ç–µ–Ω–∏—è –µ—Å–ª–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ —Ñ–∞–π–ª–∞ –æ—Ç–ª–∏—á–∞–µ—Ç—Å—è
        try:
            with open(csv_filename, 'r', encoding='utf-8') as f:
                lines = f.readlines()
                for line in lines:
                    if ';' in line:
                        parts = line.strip().split(';')
                        if len(parts) >= 2:
                            full_name = parts[0].strip()
                            short_forms_str = parts[1].strip()
                            short_forms = [s.strip() for s in short_forms_str.split(',')]
                            for short in short_forms:
                                if short:
                                    abbreviations[short] = full_name
        except Exception as e2:
            print(f"Alternative loading also failed: {e2}")

    load_time = time.time() - start_time
    print(f" –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(abbreviations)} —Å–æ–∫—Ä–∞—â–µ–Ω–∏–π –∑–∞ {load_time:.2f} —Å–µ–∫—É–Ω–¥")
    return abbreviations

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ —Ä–µ–≥—É–ª—è—Ä–Ω—ã—Ö –≤—ã—Ä–∞–∂–µ–Ω–∏–π
def create_abbreviation_patterns(abbreviations_dict):
    """
    –°–æ–∑–¥–∞–µ—Ç –ø–∞—Ç—Ç–µ—Ä–Ω—ã —Ä–µ–≥—É–ª—è—Ä–Ω—ã—Ö –≤—ã—Ä–∞–∂–µ–Ω–∏–π –¥–ª—è –≤—Å–µ—Ö —Å–æ–∫—Ä–∞—â–µ–Ω–∏–π
    """
    print(f"üîß –°–æ–∑–¥–∞–Ω–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –¥–ª—è {len(abbreviations_dict)} —Å–æ–∫—Ä–∞—â–µ–Ω–∏–π...")
    patterns = {}

    for short_form, full_name in abbreviations_dict.items():
        if '.' in short_form:
            # –î–ª—è —Å–æ–∫—Ä–∞—â–µ–Ω–∏–π —Å —Ç–æ—á–∫–æ–π: –∏—â–µ–º —Å —Ç–æ—á–∫–æ–π, –ø–æ—Å–ª–µ –∫–æ—Ç–æ—Ä–æ–π –º–æ–∂–µ—Ç –±—ã—Ç—å –ø—Ä–æ–±–µ–ª –∏–ª–∏ –∫–æ–Ω–µ—Ü —Å—Ç—Ä–æ–∫–∏
            pattern = r'\b' + re.escape(short_form) + r'(?=\s|$|\.|,|;)'
        else:
            # –î–ª—è —Å–æ–∫—Ä–∞—â–µ–Ω–∏–π –±–µ–∑ —Ç–æ—á–∫–∏: –æ–±—ã—á–Ω—ã–π –ø–æ–∏—Å–∫ —Å –≥—Ä–∞–Ω–∏—Ü–∞–º–∏ —Å–ª–æ–≤
            pattern = r'\b' + re.escape(short_form) + r'\b'
        patterns[short_form] = (pattern, full_name)

    print(f" –ü–∞—Ç—Ç–µ—Ä–Ω—ã —Å–æ–∑–¥–∞–Ω—ã –¥–ª—è {len(patterns)} —Å–æ–∫—Ä–∞—â–µ–Ω–∏–π")
    return patterns

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–æ–∏—Å–∫–∞ —Å–æ–∫—Ä–∞—â–µ–Ω–∏–π –≤ –æ–∫—Ä–µ—Å—Ç–Ω–æ—Å—Ç–∏ —Ç–æ–ø–æ–Ω–∏–º–∞
def find_nearby_abbreviations(text, toponym, patterns, distance_words=2):
    """
    –ò—â–µ—Ç —Å–æ–∫—Ä–∞—â–µ–Ω–∏—è –≤ –æ–∫—Ä–µ—Å—Ç–Ω–æ—Å—Ç–∏ —Ç–æ–ø–æ–Ω–∏–º–∞ –∏—Å–ø–æ–ª—å–∑—É—è –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ —Å–æ–∑–¥–∞–Ω–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
    """
    search_start_time = time.time()
    nearby_abbreviations = []

    # –ù–∞—Ö–æ–¥–∏–º –≤—Å–µ –≤—Ö–æ–∂–¥–µ–Ω–∏—è —Ç–æ–ø–æ–Ω–∏–º–∞ –≤ —Ç–µ–∫—Å—Ç–µ
    toponym_search_start = time.time()
    toponym_pattern = re.escape(toponym)
    toponym_matches = list(re.finditer(toponym_pattern, text))
    toponym_search_time = time.time() - toponym_search_start
    print(f"    –ü–æ–∏—Å–∫ —Ç–æ–ø–æ–Ω–∏–º–∞ '{toponym}': {toponym_search_time:.4f} —Å–µ–∫, –Ω–∞–π–¥–µ–Ω–æ {len(toponym_matches)} –≤—Ö–æ–∂–¥–µ–Ω–∏–π")

    for toponym_match in toponym_matches:
        toponym_start = toponym_match.start()
        toponym_end = toponym_match.end()

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –æ–∫—Ä–µ—Å—Ç–Ω–æ—Å—Ç—å –≤–æ–∫—Ä—É–≥ —Ç–æ–ø–æ–Ω–∏–º–∞ (–≤ —Å–∏–º–≤–æ–ª–∞—Ö)
        # –ü—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω–æ: —Å—á–∏—Ç–∞–µ–º —á—Ç–æ –≤ —Å—Ä–µ–¥–Ω–µ–º —Å–ª–æ–≤–æ = 6 —Å–∏–º–≤–æ–ª–æ–≤ + –ø—Ä–æ–±–µ–ª
        approx_distance_chars = distance_words * 7

        search_start = max(0, toponym_start - approx_distance_chars)
        search_end = min(len(text), toponym_end + approx_distance_chars)

        search_area = text[search_start:search_end]

        # –ò—â–µ–º –≤—Å–µ —Å–æ–∫—Ä–∞—â–µ–Ω–∏—è –≤ —ç—Ç–æ–π –æ–±–ª–∞—Å—Ç–∏
        abbreviation_search_start = time.time()
        for short_form, (pattern, full_name) in patterns.items():
            abbreviation_matches = list(re.finditer(pattern, search_area))

            for abbr_match in abbreviation_matches:
                # –í—ã—á–∏—Å–ª—è–µ–º –∞–±—Å–æ–ª—é—Ç–Ω—É—é –ø–æ–∑–∏—Ü–∏—é —Å–æ–∫—Ä–∞—â–µ–Ω–∏—è –≤ —Ç–µ–∫—Å—Ç–µ
                abbr_absolute_pos = search_start + abbr_match.start()

                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ —Å–æ–∫—Ä–∞—â–µ–Ω–∏–µ –Ω–µ —è–≤–ª—è–µ—Ç—Å—è —á–∞—Å—Ç—å—é —Å–∞–º–æ–≥–æ —Ç–æ–ø–æ–Ω–∏–º–∞
                if not (toponym_start <= abbr_absolute_pos <= toponym_end):
                    abbr_info = {
                        'abbreviation': abbr_match.group(),
                        'full_name': full_name,
                        'position': abbr_absolute_pos,
                        'context': search_area
                    }

                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –¥—É–±–ª–∏–∫–∞—Ç—ã
                    is_duplicate = False
                    for existing in nearby_abbreviations:
                        if (existing['abbreviation'] == abbr_info['abbreviation'] and
                            existing['full_name'] == abbr_info['full_name'] and
                            abs(existing['position'] - abbr_info['position']) < 10):
                            is_duplicate = True
                            break

                    if not is_duplicate:
                        nearby_abbreviations.append(abbr_info)

        abbreviation_search_time = time.time() - abbreviation_search_start
        print(f"    –ü–æ–∏—Å–∫ —Å–æ–∫—Ä–∞—â–µ–Ω–∏–π –≤–æ–∫—Ä—É–≥ '{toponym}': {abbreviation_search_time:.4f} —Å–µ–∫")

    total_search_time = time.time() - search_start_time
    if nearby_abbreviations:
        print(f"    –ù–∞–π–¥–µ–Ω–æ {len(nearby_abbreviations)} —Å–æ–∫—Ä–∞—â–µ–Ω–∏–π –∑–∞ {total_search_time:.4f} —Å–µ–∫")
    else:
        print(f"    –°–æ–∫—Ä–∞—â–µ–Ω–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –∑–∞ {total_search_time:.4f} —Å–µ–∫")

    return nearby_abbreviations

def main(max_lines):
    """
    –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Å—Ç—Ä–æ–∫

    Args:
        max_lines (int): –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–æ–∫ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏. –ï—Å–ª–∏ None - –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –≤—Å–µ.
    """
    total_start_time = time.time()

    # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å–æ–∫—Ä–∞—â–µ–Ω–∏—è
    abbreviations_dict = load_abbreviations('abbreviations.csv')

    # –°–æ–∑–¥–∞–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã —Ä–µ–≥—É–ª—è—Ä–Ω—ã—Ö –≤—ã—Ä–∞–∂–µ–Ω–∏–π –æ–¥–∏–Ω —Ä–∞–∑ –¥–ª—è –≤—Å–µ—Ö –≤—ã–∑–æ–≤–æ–≤
    patterns_creation_start = time.time()
    patterns = create_abbreviation_patterns(abbreviations_dict)
    patterns_creation_time = time.time() - patterns_creation_start
    print(f"‚è± –û–±—â–µ–µ –≤—Ä–µ–º—è —Å–æ–∑–¥–∞–Ω–∏—è –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤: {patterns_creation_time:.4f} —Å–µ–∫")

    lines = df['main_text'].tolist()
    print(f"\n –í—Å–µ–≥–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –≤ –¥–∞–Ω–Ω—ã—Ö: {len(lines)}")

    # –ü—Ä–∏–º–µ–Ω—è–µ–º –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É —Å—Ç—Ä–æ–∫
    if max_lines is not None:
        lines = lines[:max_lines]
        print(f" –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã—Ö {max_lines} —Å—Ç—Ä–æ–∫")

    df['main_text'] = df['main_text'].replace({float('nan'): ""})

    num = 0
    output_csv = 'output.csv'

    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    total_toponyms = 0
    total_abbreviations = 0
    processed_sentences = 0

    # –í—Ä–µ–º—è –¥–ª—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
    total_toponym_time = 0
    total_abbreviation_time = 0
    total_nlp_time = 0

    print(f"\n –ù–∞—á–∞–ª–æ –æ–±—Ä–∞–±–æ—Ç–∫–∏ {len(lines)} –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π...")
    processing_start_time = time.time()

    with open(output_csv, 'w', encoding='utf-8') as outfile:
        outfile.writelines(f"sentence_id; toponyms_list; nearby_abbreviations \n")

        # –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø: –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –±–∞—Ç—á–∞–º–∏ –¥–ª—è –ª—É—á—à–µ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        batch_size = 100  # –ú–æ–∂–Ω–æ –Ω–∞—Å—Ç—Ä–æ–∏—Ç—å –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –æ–±—ä–µ–º–∞ –ø–∞–º—è—Ç–∏

        for i in range(0, len(lines), batch_size):
            batch_lines = lines[i:i + batch_size]
            batch_indices = list(range(i, min(i + batch_size, len(lines))))

            print(f"\n--- –û–±—Ä–∞–±–æ—Ç–∫–∞ –±–∞—Ç—á–∞ {i//batch_size + 1}/{(len(lines)-1)//batch_size + 1} ---")

            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –±–∞—Ç—á –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
            nlp_start_time = time.time()
            
            # –§–∏–ª—å—Ç—Ä—É–µ–º –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏ –ø–µ—Ä–µ–¥ –ø–µ—Ä–µ–¥–∞—á–µ–π –≤ bulk_process
            non_empty_lines = [(idx, text) for idx, text in enumerate(batch_lines) if pd.notna(text) and text != ""]
            if non_empty_lines:
                # –†–∞–∑–¥–µ–ª—è–µ–º –∏–Ω–¥–µ–∫—Å—ã –∏ —Ç–µ–∫—Å—Ç—ã
                non_empty_indices, non_empty_texts = zip(*non_empty_lines)
                
                # –í—ã–ø–æ–ª–Ω—è–µ–º bulk_process —Ç–æ–ª—å–∫–æ –¥–ª—è –Ω–µ–ø—É—Å—Ç—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤
                batch_docs = nlp.bulk_process(non_empty_texts)
                
                # –í–æ—Å—Å–æ–∑–¥–∞–µ–º –ø–æ–ª–Ω—ã–π —Å–ø–∏—Å–æ–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤, –≤–∫–ª—é—á–∞—è None –¥–ª—è –ø—É—Å—Ç—ã—Ö —Å—Ç—Ä–æ–∫
                batch_docs_full = [None] * len(batch_lines)
                for idx, doc in zip(non_empty_indices, batch_docs):
                    batch_docs_full[idx] = doc
            else:
                batch_docs_full = [None] * len(batch_lines)
                
            batch_docs = batch_docs_full
            batch_nlp_time = time.time() - nlp_start_time
            total_nlp_time += batch_nlp_time
            print(f"‚è± –í—Ä–µ–º—è NLP –æ–±—Ä–∞–±–æ—Ç–∫–∏ –±–∞—Ç—á–∞: {batch_nlp_time:.4f} —Å–µ–∫")

            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥—ã–π –¥–æ–∫—É–º–µ–Ω—Ç –≤ –±–∞—Ç—á–µ
            for j, (doc, sentence_idx) in enumerate(zip(batch_docs, batch_indices)):
                sentence = batch_lines[j]

                if doc is None:
                    # –ü—É—Å—Ç–∞—è —Å—Ç—Ä–æ–∫–∞
                    strk = f"{sentence_idx}; ; \n"
                    outfile.writelines(strk)
                    num += 1
                    continue

                sentence_start_time = time.time()

                print(f'\n--- –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ {sentence_idx} ---')
                print(f'–¢–µ–∫—Å—Ç: {sentence[:100]}...' if len(sentence) > 100 else f'–¢–µ–∫—Å—Ç: {sentence}')

                sentence_toponyms = []
                all_nearby_abbreviations = []

                # –ü–æ–∏—Å–∫ —Ç–æ–ø–æ–Ω–∏–º–æ–≤ –≤ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω–æ–º –¥–æ–∫—É–º–µ–Ω—Ç–µ
                toponym_search_start = time.time()
                for entity in doc.ents:
                    if entity.type == "LOC":
                        entity_find_time = time.time() - toponym_search_start
                        print(f'üìç –¢–æ–ø–æ–Ω–∏–º: {entity.text} (–Ω–∞–π–¥–µ–Ω –∑–∞ {entity_find_time:.4f} —Å–µ–∫)')
                        sentence_toponyms.append(entity.text)
                        total_toponyms += 1

                        # –û–±–Ω–æ–≤–ª—è–µ–º –≤—Ä–µ–º—è –¥–ª—è —Å–ª–µ–¥—É—é—â–µ–≥–æ —Ç–æ–ø–æ–Ω–∏–º–∞
                        toponym_search_start = time.time()

                        # –ò—â–µ–º —Å–æ–∫—Ä–∞—â–µ–Ω–∏—è –≤ –æ–∫—Ä–µ—Å—Ç–Ω–æ—Å—Ç–∏ —ç—Ç–æ–≥–æ —Ç–æ–ø–æ–Ω–∏–º–∞
                        abbreviation_search_start = time.time()
                        nearby_abbr = find_nearby_abbreviations_advanced(
                            sentence,
                            entity.text,
                            patterns,
                            distance_words=5
                        )
                        abbreviation_time = time.time() - abbreviation_search_start
                        total_abbreviation_time += abbreviation_time

                        print(f"    –û–±—â–µ–µ –≤—Ä–µ–º—è –ø–æ–∏—Å–∫–∞ —Å–æ–∫—Ä–∞—â–µ–Ω–∏–π –¥–ª—è '{entity.text}': {abbreviation_time:.4f} —Å–µ–∫")

                        for abbr_info in nearby_abbr:
                            match_str = f"{abbr_info['abbreviation']}"
                            if match_str not in all_nearby_abbreviations:
                                all_nearby_abbreviations.append(match_str)
                                total_abbreviations += 1
                                print(f"    –ù–∞–π–¥–µ–Ω–æ —Å–æ–∫—Ä–∞—â–µ–Ω–∏–µ: '{abbr_info['abbreviation']}' -> '{abbr_info['full_name']}'")

                # –ï—Å–ª–∏ –Ω–µ –±—ã–ª–æ –Ω–∞–π–¥–µ–Ω–æ —Ç–æ–ø–æ–Ω–∏–º–æ–≤, –≤—ã–≤–æ–¥–∏–º –≤—Ä–µ–º—è –ø–æ–∏—Å–∫–∞
                if not sentence_toponyms:
                    toponym_time = time.time() - toponym_search_start
                    total_toponym_time += toponym_time
                    print(f" –í—Ä–µ–º—è –ø–æ–∏—Å–∫–∞ —Ç–æ–ø–æ–Ω–∏–º–æ–≤: {toponym_time:.4f} —Å–µ–∫ (–Ω–µ –Ω–∞–π–¥–µ–Ω–æ)")

                # –§–æ—Ä–º–∏—Ä—É–µ–º —Å—Ç—Ä–æ–∫—É –¥–ª—è –≤—ã–≤–æ–¥–∞
                toponyms_str = ', '.join(sentence_toponyms)
                abbreviations_str = ', '.join(all_nearby_abbreviations)

                strk = f"{sentence_idx}; {toponyms_str}; {abbreviations_str} \n"
                outfile.writelines(strk)

                sentence_time = time.time() - sentence_start_time
                print(f" –û–±—â–µ–µ –≤—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è: {sentence_time:.2f} —Å–µ–∫")

                processed_sentences += 1
                num += 1

            # –ü—Ä–æ–≥—Ä–µ—Å—Å –ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–≥–æ –±–∞—Ç—á–∞
            elapsed = time.time() - processing_start_time
            print(f"\n –ü—Ä–æ–≥—Ä–µ—Å—Å: –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ {processed_sentences}/{len(lines)} –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π")
            print(f" –ü—Ä–æ—à–ª–æ –≤—Ä–µ–º–µ–Ω–∏: {timedelta(seconds=int(elapsed))}")

            # –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –ø–æ —ç—Ç–∞–ø–∞–º
            if processed_sentences > 0:
                avg_nlp_time = total_nlp_time / processed_sentences
                if total_toponyms > 0:
                    avg_abbr_time = total_abbreviation_time / total_toponyms
                    print(f" –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –ø–æ–∏—Å–∫–∞ —Å–æ–∫—Ä–∞—â–µ–Ω–∏–π –Ω–∞ —Ç–æ–ø–æ–Ω–∏–º: {avg_abbr_time:.4f} —Å–µ–∫")

                print(f" –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è NLP: {avg_nlp_time:.4f} —Å–µ–∫")

                avg_time = elapsed / processed_sentences
                remaining = (len(lines) - processed_sentences) * avg_time
                print(f" –û—Å—Ç–∞–≤—à–µ–µ—Å—è –≤—Ä–µ–º—è: ~{timedelta(seconds=int(remaining))}")

    total_time = time.time() - total_start_time

    # –§–∏–Ω–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Å –¥–µ—Ç–∞–ª–∏–∑–∞—Ü–∏–µ–π –ø–æ –≤—Ä–µ–º–µ–Ω–∏
    print(f"\n –û–ë–†–ê–ë–û–¢–ö–ê –ó–ê–í–ï–†–®–ï–ù–ê!")
    print(f" –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
    print(f"   - –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π: {processed_sentences}")
    print(f"   - –ù–∞–π–¥–µ–Ω–æ —Ç–æ–ø–æ–Ω–∏–º–æ–≤: {total_toponyms}")
    print(f"   - –ù–∞–π–¥–µ–Ω–æ —Å–æ–∫—Ä–∞—â–µ–Ω–∏–π: {total_abbreviations}")
    print(f"   - –û–±—â–µ–µ –≤—Ä–µ–º—è: {timedelta(seconds=int(total_time))}")

    if processed_sentences > 0:
        print(f"\n‚è± –î–µ—Ç–∞–ª–∏–∑–∞—Ü–∏—è –≤—Ä–µ–º–µ–Ω–∏:")
        print(f"   - –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ: {total_time/processed_sentences:.2f} —Å–µ–∫")
        print(f"   - –û–±—â–µ–µ –≤—Ä–µ–º—è NLP: {timedelta(seconds=int(total_nlp_time))}")
        print(f"   - –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è NLP –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ: {total_nlp_time/processed_sentences:.4f} —Å–µ–∫")
        print(f"   - –í—Ä–µ–º—è —Å–æ–∑–¥–∞–Ω–∏—è –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤: {patterns_creation_time:.4f} —Å–µ–∫")

        if total_toponyms > 0:
            print(f"   - –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –ø–æ–∏—Å–∫–∞ —Å–æ–∫—Ä–∞—â–µ–Ω–∏–π –Ω–∞ —Ç–æ–ø–æ–Ω–∏–º: {total_abbreviation_time/total_toponyms:.4f} —Å–µ–∫")
            print(f"   - –°—Ä–µ–¥–Ω–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–ø–æ–Ω–∏–º–æ–≤ –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ: {total_toponyms/processed_sentences:.2f}")
            print(f"   - –°—Ä–µ–¥–Ω–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–æ–∫—Ä–∞—â–µ–Ω–∏–π –Ω–∞ —Ç–æ–ø–æ–Ω–∏–º: {total_abbreviations/max(total_toponyms, 1):.2f}")

    print(f"   - –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {output_csv}")
# –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω–∞—è –≤–µ—Ä—Å–∏—è —Å –±–æ–ª–µ–µ —Ç–æ—á–Ω—ã–º –ø–æ–∏—Å–∫–æ–º –ø–æ —Å–ª–æ–≤–∞–º
def find_nearby_abbreviations_advanced(text, toponym, patterns, distance_words=2):
    """
    –£–ª—É—á—à–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è –ø–æ–∏—Å–∫–∞ —Å —É—á–µ—Ç–æ–º —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è –≤ —Å–ª–æ–≤–∞—Ö
    """
    search_start_time = time.time()
    nearby_abbreviations = []

    # –¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –ø–æ–∑–∏—Ü–∏–π
    tokenization_start = time.time()
    words_with_positions = []
    pattern = r'\b\w+(?:\.\w+)*\b|[^\w\s]'
    for match in re.finditer(pattern, text):
        word = match.group()
        if word.strip():  # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º —á–∏—Å—Ç—ã–µ –ø—Ä–æ–±–µ–ª—ã
            words_with_positions.append({
                'word': word,
                'start': match.start(),
                'end': match.end()
            })
    tokenization_time = time.time() - tokenization_start
    print(f"    –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞: {tokenization_time:.4f} —Å–µ–∫")

    # –ù–∞—Ö–æ–¥–∏–º –ø–æ–∑–∏—Ü–∏–∏ —Ç–æ–ø–æ–Ω–∏–º–∞
    toponym_search_start = time.time()
    toponym_positions = []
    for i, word_info in enumerate(words_with_positions):
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ—Ç–¥–µ–ª—å–Ω—ã–µ —Å–ª–æ–≤–∞ –∏ –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏ —Å–ª–æ–≤ –¥–ª—è —Ç–æ–ø–æ–Ω–∏–º–∞
        if word_info['word'] in toponym:
            # –ü—Ä–æ—Å—Ç–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ - –º–æ–∂–Ω–æ —É–ª—É—á—à–∏—Ç—å –¥–ª—è –º–Ω–æ–≥–æ—Å–ª–æ–≤–Ω—ã—Ö —Ç–æ–ø–æ–Ω–∏–º–æ–≤
            toponym_positions.append(i)
    toponym_search_time = time.time() - toponym_search_start
    print(f"    –ü–æ–∏—Å–∫ –ø–æ–∑–∏—Ü–∏–π —Ç–æ–ø–æ–Ω–∏–º–∞: {toponym_search_time:.4f} —Å–µ–∫, –Ω–∞–π–¥–µ–Ω–æ {len(toponym_positions)} –ø–æ–∑–∏—Ü–∏–π")

    # –î–ª—è –∫–∞–∂–¥–æ–π –ø–æ–∑–∏—Ü–∏–∏ —Ç–æ–ø–æ–Ω–∏–º–∞ –∏—â–µ–º —Å–æ—Å–µ–¥–Ω–∏–µ —Å–æ–∫—Ä–∞—â–µ–Ω–∏—è
    abbreviation_search_start = time.time()
    for pos in toponym_positions:
        start_idx = max(0, pos - distance_words)
        end_idx = min(len(words_with_positions), pos + distance_words + 1)

        for i in range(start_idx, end_idx):
            word_info = words_with_positions[i]
            word = word_info['word']

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Å–ª–æ–≤–æ —Å–æ–∫—Ä–∞—â–µ–Ω–∏–µ–º
            for short_form, (pattern, full_name) in patterns.items():
                # –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º —Å –æ—á–∏—â–µ–Ω–Ω–æ–π –≤–µ—Ä—Å–∏–µ–π —Å–æ–∫—Ä–∞—â–µ–Ω–∏—è
                clean_short = short_form.replace('.', '')
                clean_word = word.replace('.', '')

                if clean_word == clean_short:
                    abbr_info = {
                        'abbreviation': word,
                        'full_name': full_name,
                        'position': word_info['start']
                    }

                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –¥—É–±–ª–∏–∫–∞—Ç—ã
                    is_duplicate = False
                    for existing in nearby_abbreviations:
                        if (existing['abbreviation'] == abbr_info['abbreviation'] and
                            existing['full_name'] == abbr_info['full_name']):
                            is_duplicate = True
                            break

                    if not is_duplicate:
                        nearby_abbreviations.append(abbr_info)

    abbreviation_search_time = time.time() - abbreviation_search_start
    total_search_time = time.time() - search_start_time

    print(f"    –ü–æ–∏—Å–∫ —Å–æ–∫—Ä–∞—â–µ–Ω–∏–π: {abbreviation_search_time:.4f} —Å–µ–∫")
    if nearby_abbreviations:
        print(f"    –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –ø–æ–∏—Å–∫: –Ω–∞–π–¥–µ–Ω–æ {len(nearby_abbreviations)} —Å–æ–∫—Ä–∞—â–µ–Ω–∏–π –∑–∞ {total_search_time:.4f} —Å–µ–∫")
    else:
        print(f"    –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –ø–æ–∏—Å–∫: —Å–æ–∫—Ä–∞—â–µ–Ω–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –∑–∞ {total_search_time:.4f} —Å–µ–∫")

    return nearby_abbreviations

# –ó–∞–ø—É—Å–∫ –æ—Å–Ω–æ–≤–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–∏
if __name__ == "__main__":
    # –û–±—Ä–∞–±–æ—Ç–∞—Ç—å —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ 50 —Å—Ç—Ä–æ–∫ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
    #main(max_lines=100)

    # –ò–ª–∏ –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å –≤—Å–µ —Å—Ç—Ä–æ–∫–∏:
    main(None)