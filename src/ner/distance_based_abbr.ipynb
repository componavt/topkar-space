{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/componavt/topkar-space/blob/main/src/ner/distance_based_abbr.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LUDjzJQsMQzH"
      },
      "outputs": [],
      "source": [
        "def analyze_and_save_simple():\n",
        "    \"\"\"\n",
        "    –£–ø—Ä–æ—â–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\n",
        "    \"\"\"\n",
        "    abbreviations = load_abbreviations('abbreviations.csv')\n",
        "\n",
        "    # with open('output_line_by_line.txt', 'r', encoding='utf-8') as file:\n",
        "    #     lines = file.readlines()\n",
        "    lines = df\n",
        "    results = []\n",
        "\n",
        "    for line_num, line in enumerate(lines, 1):\n",
        "        words = re.findall(r'\\b[\\w.-]+(?:\\.\\w*)?\\b', line)\n",
        "\n",
        "        for word in words:\n",
        "            if word in abbreviations:\n",
        "                results.append(f\"–°—Ç—Ä–æ–∫–∞ {line_num}: '{word}' -> {abbreviations[word]}\\n–ö–æ–Ω—Ç–µ–∫—Å—Ç: {line.strip()}\\n{'-'*50}\")\n",
        "\n",
        "    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ —Ñ–∞–π–ª\n",
        "    with open('res.txt', 'w', encoding='utf-8') as f:\n",
        "        f.write(f\"–ù–∞–π–¥–µ–Ω–æ —Å–æ–∫—Ä–∞—â–µ–Ω–∏–π: {len(results)}\\n\\n\")\n",
        "        f.write(\"\\n\".join(results))\n",
        "\n",
        "    print(f\"–†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ —Ñ–∞–π–ª '–Ω–∞–π–¥–µ–Ω–Ω—ã–µ_—Å–æ–∫—Ä–∞—â–µ–Ω–∏—è.txt'\")\n",
        "    print(f\"–í—Å–µ–≥–æ –Ω–∞–π–¥–µ–Ω–æ: {len(results)} —Å–æ–∫—Ä–∞—â–µ–Ω–∏–π\")\n",
        "\n",
        "# –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –∑–∞–ø—É—Å–∫\n",
        "analyze_and_save_simple()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-leUPH5TwVGL"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "csv_files = [\n",
        "    #\"https://raw.githubusercontent.com/componavt/topkar-space/main/data/abbreviations.csv\",\n",
        "    \"https://raw.githubusercontent.com/componavt/topkar-space/main/data/toponims_fix.csv\",\n",
        "]\n",
        "\n",
        "df = pd.concat([pd.read_csv(url, sep = ';') for url in csv_files], ignore_index=True)\n",
        "df = df.reset_index()  # make sure indexes pair with number of rows\n",
        "df.head()\n",
        "\n",
        "csv_files = [\n",
        "    \"https://raw.githubusercontent.com/componavt/topkar-space/main/data/abbreviations.csv\",\n",
        "]\n",
        "\n",
        "dd = pd.concat([pd.read_csv(url, sep = ';') for url in csv_files], ignore_index=True)\n",
        "dd = dd.reset_index()  # make sure indexes pair with number of rows\n",
        "dd.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UCSwbVhCCQp3"
      },
      "outputs": [],
      "source": [
        "def remove_extra_semicolons_total(text, max_semicolons=3):\n",
        "    \"\"\"\n",
        "    –£–¥–∞–ª—è–µ—Ç –ª–∏—à–Ω–∏–µ —Ç–æ—á–∫–∏ —Å –∑–∞–ø—è—Ç–æ–π, —á—Ç–æ–±—ã –æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–µ –ø—Ä–µ–≤—ã—à–∞–ª–æ max_semicolons\n",
        "    –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –ø–µ—Ä–≤—ã–µ –≤—Å—Ç—Ä–µ—á–µ–Ω–Ω—ã–µ —Ç–æ—á–∫–∏ —Å –∑–∞–ø—è—Ç–æ–π\n",
        "    \"\"\"\n",
        "    result = []\n",
        "    count = 0\n",
        "\n",
        "    for char in text:\n",
        "        if char == ';':\n",
        "            if count < max_semicolons:\n",
        "                result.append(char)\n",
        "                count += 1\n",
        "            # else: –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –ª–∏—à–Ω—é—é —Ç–æ—á–∫—É —Å –∑–∞–ø—è—Ç–æ–π\n",
        "        else:\n",
        "            result.append(char)\n",
        "\n",
        "    return ''.join(result)\n",
        "\n",
        "def process_file_line_by_line(input_file, output_file, max_semicolons=3):\n",
        "    \"\"\"\n",
        "    –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —Ñ–∞–π–ª –ø–æ—Å—Ç—Ä–æ—á–Ω–æ (–ø–æ–ª–µ–∑–Ω–æ –¥–ª—è –±–æ–ª—å—à–∏—Ö —Ñ–∞–π–ª–æ–≤)\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with open(input_file, 'r', encoding='utf-8') as infile, \\\n",
        "             open(output_file, 'w', encoding='utf-8') as outfile:\n",
        "\n",
        "            for line in infile:\n",
        "                processed_line = remove_extra_semicolons_total(line, max_semicolons)\n",
        "                outfile.write(processed_line)\n",
        "\n",
        "        print(f\"–§–∞–π–ª —É—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω –ø–æ—Å—Ç—Ä–æ—á–Ω–æ!\")\n",
        "        print(f\"–†–µ–∑—É–ª—å—Ç–∞—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤: {output_file}\")\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"–û—à–∏–±–∫–∞: –§–∞–π–ª '{input_file}' –Ω–µ –Ω–∞–π–¥–µ–Ω!\")\n",
        "    except Exception as e:\n",
        "        print(f\"–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞: {e}\")\n",
        "\n",
        "# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–æ—Å—Ç—Ä–æ—á–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏\n",
        "if __name__ == \"__main__\":\n",
        "    input_filename = \"toponyms.csv\"\n",
        "    output_filename = \"output_line_by_line.txt\"\n",
        "\n",
        "    process_file_line_by_line(input_filename, output_filename)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LQUH0pzOPgXS"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import re\n",
        "from datetime import datetime\n",
        "import requests\n",
        "from io import StringIO\n",
        "\n",
        "def load_file_from_github(filename, max_lines=None):\n",
        "    \"\"\"–ö–æ–º–ø–∞–∫—Ç–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ —Ñ–∞–π–ª–∞ —Å GitHub —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º —Å—Ç—Ä–æ–∫\"\"\"\n",
        "    url = f\"https://raw.githubusercontent.com/componavt/topkar-space/main/data/{filename}\"\n",
        "    response = requests.get(url)\n",
        "    response.raise_for_status()\n",
        "\n",
        "    text = response.text\n",
        "    if max_lines is not None:\n",
        "        lines = text.split('\\n')[:max_lines]\n",
        "        text = '\\n'.join(lines)\n",
        "        print(f\"–ó–∞–≥—Ä—É–∂–µ–Ω–æ –ø–µ—Ä–≤—ã—Ö {max_lines} —Å—Ç—Ä–æ–∫ –∏–∑ {filename}\")\n",
        "\n",
        "    return text\n",
        "\n",
        "def load_abbreviations(csv_filename):\n",
        "    \"\"\"\n",
        "    –ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å–æ–∫—Ä–∞—â–µ–Ω–∏—è –∏–∑ CSV —Ñ–∞–π–ª–∞\n",
        "    \"\"\"\n",
        "    abbreviations = {}\n",
        "\n",
        "    # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ñ–∞–π–ª —Å GitHub\n",
        "    csv_content = load_file_from_github(csv_filename)\n",
        "    csvfile = StringIO(csv_content)\n",
        "\n",
        "    reader = csv.DictReader(csvfile, delimiter=';')\n",
        "    for row in reader:\n",
        "        full_name = row['–û–±–æ–∑–Ω–∞—á–µ–Ω–∏–µ']\n",
        "        short_forms_str = row['–°–æ–∫—Ä–∞—â–µ–Ω–∏—è']\n",
        "\n",
        "        if short_forms_str:\n",
        "            short_forms = [s.strip() for s in short_forms_str.split(',')]\n",
        "            for short in short_forms:\n",
        "                if short:\n",
        "                    abbreviations[short] = full_name\n",
        "                    print(f\"–î–æ–±–∞–≤–ª–µ–Ω–æ —Å–æ–∫—Ä–∞—â–µ–Ω–∏–µ: '{short}' -> {full_name}\")\n",
        "\n",
        "    print(f\"\\n–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(abbreviations)} —Å–æ–∫—Ä–∞—â–µ–Ω–∏–π\")\n",
        "    return abbreviations\n",
        "\n",
        "def analyze_file_with_abbreviations(max_lines=None):\n",
        "    \"\"\"\n",
        "    –§—É–Ω–∫—Ü–∏—è –¥–ª—è –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ —Ñ–∞–π–ª–∞ —Å –ø–æ–¥—Å—á–µ—Ç–æ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏\n",
        "    \"\"\"\n",
        "    # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å–æ–∫—Ä–∞—â–µ–Ω–∏—è\n",
        "    abbreviations = load_abbreviations('abbreviations.csv')\n",
        "\n",
        "    # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–µ–∫—Å—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Å GitHub\n",
        "    text = load_file_from_github('toponims_fix.csv', max_lines)\n",
        "\n",
        "    total_matches = 0\n",
        "    matches_by_type = {}\n",
        "    all_matches = []\n",
        "\n",
        "    # –ò—â–µ–º –≤—Å–µ –≤–æ–∑–º–æ–∂–Ω—ã–µ —Å–æ–∫—Ä–∞—â–µ–Ω–∏—è –≤ —Ç–µ–∫—Å—Ç–µ\n",
        "    for short_form, full_name in abbreviations.items():\n",
        "        if '.' in short_form:\n",
        "            pattern = r'\\b' + re.escape(short_form) + r'(?=\\s|$|\\.|,|;)'\n",
        "        else:\n",
        "            pattern = r'\\b' + re.escape(short_form) + r'\\b'\n",
        "\n",
        "        matches = re.finditer(pattern, text)\n",
        "\n",
        "        for match in matches:\n",
        "            total_matches += 1\n",
        "            line_num = text[:match.start()].count('\\n') + 1\n",
        "\n",
        "            line_start = text.rfind('\\n', 0, match.start()) + 1\n",
        "            line_end = text.find('\\n', match.start())\n",
        "            if line_end == -1:\n",
        "                line_end = len(text)\n",
        "            context = text[line_start:line_end].strip()\n",
        "\n",
        "            if full_name not in matches_by_type:\n",
        "                matches_by_type[full_name] = []\n",
        "\n",
        "            match_info = {\n",
        "                'line': line_num,\n",
        "                'word': match.group(),\n",
        "                'full_name': full_name,\n",
        "                'context': context\n",
        "            }\n",
        "\n",
        "            matches_by_type[full_name].append(match_info)\n",
        "            all_matches.append(match_info)\n",
        "\n",
        "    all_matches.sort(key=lambda x: x['line'])\n",
        "    save_results_to_file(all_matches, matches_by_type, total_matches)\n",
        "\n",
        "    # –í—ã–≤–æ–¥ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä\n",
        "    print(f\"\\nüìä –í—Å–µ–≥–æ –Ω–∞–π–¥–µ–Ω–æ –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä: {total_matches}\")\n",
        "    print(\"\\nüìã –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ —Ç–∏–ø–∞–º:\")\n",
        "    for geo_type, matches in sorted(matches_by_type.items(), key=lambda x: len(x[1]), reverse=True):\n",
        "        print(f\"  {geo_type}: {len(matches)}\")\n",
        "\n",
        "    return matches_by_type\n",
        "\n",
        "def save_results_to_file(all_matches, matches_by_type, total_matches):\n",
        "    \"\"\"\n",
        "    –°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ —Ñ–∞–π–ª\n",
        "    \"\"\"\n",
        "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "    filename = f\"results_{timestamp}.txt\"\n",
        "\n",
        "    with open(filename, 'w', encoding='utf-8') as f:\n",
        "        for match in all_matches:\n",
        "            f.write(f\"{match['context']};{match['word']}\\n\")\n",
        "\n",
        "    print(f\"\\nüíæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ —Ñ–∞–π–ª: {filename}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # –ó–∞–ø—É—Å–∫ —Ç–µ—Å—Ç–∞\n",
        "    print(\"–ó–∞–ø—É—Å–∫ —Ç–µ—Å—Ç–∞ –ø–æ–∏—Å–∫–∞ —Å–æ–∫—Ä–∞—â–µ–Ω–∏–π...\")\n",
        "    print(\"\\n\" + \"=\" * 50 + \"\\n\")\n",
        "\n",
        "    # –û–±—Ä–∞–±–æ—Ç–∞—Ç—å —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ 100 —Å—Ç—Ä–æ–∫\n",
        "    analyze_file_with_abbreviations(max_lines=100)\n",
        "\n",
        "    # –ò–ª–∏ –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å –≤—Å–µ —Å—Ç—Ä–æ–∫–∏:\n",
        "    # analyze_file_with_abbreviations()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yvyyqtL3vHKS"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# –ß—Ç–µ–Ω–∏–µ —Ñ–∞–π–ª–∞ (–ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ—Ç—Å—è, —á—Ç–æ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å - —Ç–æ—á–∫–∞ —Å –∑–∞–ø—è—Ç–æ–π)\n",
        "df = pd.read_csv('results.txt', sep=';', header=None,\n",
        "                 names=['id', 'name_ru', 'name_en', 'description', 'abbreviation'])\n",
        "\n",
        "# –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ id –∏ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Å–æ–∫—Ä–∞—â–µ–Ω–∏–π\n",
        "result = df.groupby('id').agg({\n",
        "    'name_ru': 'first',\n",
        "    'name_en': 'first',\n",
        "    'description': 'first',\n",
        "    'abbreviation': lambda x: ', '.join(x)\n",
        "}).reset_index()\n",
        "\n",
        "# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞\n",
        "result.to_csv('combined_result.csv', sep=';', index=False)\n",
        "\n",
        "print(\"–†–µ–∑—É–ª—å—Ç–∞—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ —Ñ–∞–π–ª 'combined_result.csv'\")\n",
        "print(\"\\n–†–µ–∑—É–ª—å—Ç–∞—Ç:\")\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uDOuDk9dDx6j"
      },
      "outputs": [],
      "source": [
        "!pip install stanza\n",
        "import stanza\n",
        "\n",
        "# –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞\n",
        "stanza.download('ru')  # –ó–∞–≥—Ä—É–∑–∏—Ç–µ –º–æ–¥–µ–ª—å, –µ—Å–ª–∏ –æ–Ω–∞ –µ—â–µ –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞\n",
        "nlp = stanza.Pipeline('ru', processors='tokenize,ner')\n",
        "# –ü—Ä–∏–º–µ—Ä —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ\n",
        "text = \"–î–æ–º –≤ –¥–µ—Ä. –ö–æ–π–≤—É—Å–µ–ª—å–≥–∞, —Ñ–∞–º. –ú–∏—Ö–∞–π–ª–æ–≤. =\"\n",
        "\n",
        "# –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ç–µ–∫—Å—Ç\n",
        "doc = nlp(text)\n",
        "\n",
        "# –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–º–µ–Ω–æ–≤–∞–Ω–Ω—ã–µ —Å—É—â–Ω–æ—Å—Ç–∏\n",
        "for sentence in doc.sentences:\n",
        "    for entity in sentence.ents:\n",
        "        print(f'–¢–µ–∫—Å—Ç: {entity.text}, –¢–∏–ø: {entity.type}')\n",
        "\n",
        "text = \"–Ø–±–ª–æ–∫–æ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –ø–æ–∫—É–ø–∫–∏ —Å—Ç–∞—Ä—Ç–∞–ø–∞ –≤ –í–µ–ª–∏–∫–æ–±—Ä–∏—Ç–∞–Ω–∏–∏ –∑–∞ 1 –º–∏–ª–ª–∏–∞—Ä–¥ –¥–æ–ª–ª–∞—Ä–æ–≤. –í–ª–∞–¥–∏–º–∏—Ä –ü—É—Ç–∏–Ω —è–≤–ª—è–µ—Ç—Å—è –ø—Ä–µ–∑–∏–¥–µ–Ω—Ç–æ–º –†–æ—Å—Å–∏–∏. –ê –µ—â—ë –ø–∞—Å—Ç–∞ –∏ –ö–∏–∂–∏\"\n",
        "\n",
        "doc = nlp(text)\n",
        "\n",
        "for sentence in doc.sentences:\n",
        "    for entity in sentence.ents:\n",
        "        print(f'–¢–µ–∫—Å—Ç: {entity.text}, –¢–∏–ø: {entity.type}')\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# csv_files = [\n",
        "# #    \"https://raw.githubusercontent.com/componavt/topkar-space/main/data/sample10.csv\",\n",
        "#     \"https://raw.githubusercontent.com/componavt/topkar-space/main/data/toponims_fix.csv\",\n",
        "# ]\n",
        "csv_files = [\n",
        "#    \"https://raw.githubusercontent.com/componavt/topkar-space/main/data/sample10.csv\",\n",
        "    \"https://raw.githubusercontent.com/componavt/topkar-space/main/data/reference_abr_sample100.csv\",\n",
        "]\n",
        "df = pd.concat([pd.read_csv(url, sep = ',') for url in csv_files], ignore_index=True)\n",
        "df = df.reset_index()  # make sure indexes pair with number of rows\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yNacE88YJMOg"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "import pandas as pd\n",
        "import re\n",
        "import csv\n",
        "from ast import Import\n",
        "import nltk\n",
        "import re\n",
        "import time\n",
        "from datetime import timedelta\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "\n",
        "# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ —Å–æ–∫—Ä–∞—â–µ–Ω–∏–π –∏–∑ CSV —Ñ–∞–π–ª–∞\n",
        "def load_abbreviations(csv_filename):\n",
        "    \"\"\"\n",
        "    –ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å–æ–∫—Ä–∞—â–µ–Ω–∏—è –∏–∑ CSV —Ñ–∞–π–ª–∞\n",
        "    \"\"\"\n",
        "    print(f\"‚è≥ –ó–∞–≥—Ä—É–∑–∫–∞ —Å–æ–∫—Ä–∞—â–µ–Ω–∏–π –∏–∑ {csv_filename}...\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    abbreviations = {}\n",
        "    try:\n",
        "        with open(csv_filename, 'r', encoding='utf-8') as csvfile:\n",
        "            reader = csv.DictReader(csvfile, delimiter=';')\n",
        "            for row in reader:\n",
        "                full_name = row['–û–±–æ–∑–Ω–∞—á–µ–Ω–∏–µ']\n",
        "                short_forms_str = row['–°–æ–∫—Ä–∞—â–µ–Ω–∏—è']\n",
        "\n",
        "                if short_forms_str:\n",
        "                    # –†–∞–∑–¥–µ–ª—è–µ–º –ø–æ –∑–∞–ø—è—Ç–æ–π –∏ —É–±–∏—Ä–∞–µ–º –ø—Ä–æ–±–µ–ª—ã\n",
        "                    short_forms = [s.strip() for s in short_forms_str.split(',')]\n",
        "\n",
        "                    for short in short_forms:\n",
        "                        if short:\n",
        "                            abbreviations[short] = full_name\n",
        "                            print(f\"–î–æ–±–∞–≤–ª–µ–Ω–æ —Å–æ–∫—Ä–∞—â–µ–Ω–∏–µ: '{short}' -> {full_name}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading abbreviations: {e}\")\n",
        "        # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π —Å–ø–æ—Å–æ–± —á—Ç–µ–Ω–∏—è –µ—Å–ª–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ —Ñ–∞–π–ª–∞ –æ—Ç–ª–∏—á–∞–µ—Ç—Å—è\n",
        "        try:\n",
        "            with open(csv_filename, 'r', encoding='utf-8') as f:\n",
        "                lines = f.readlines()\n",
        "                for line in lines:\n",
        "                    if ';' in line:\n",
        "                        parts = line.strip().split(';')\n",
        "                        if len(parts) >= 2:\n",
        "                            full_name = parts[0].strip()\n",
        "                            short_forms_str = parts[1].strip()\n",
        "                            short_forms = [s.strip() for s in short_forms_str.split(',')]\n",
        "                            for short in short_forms:\n",
        "                                if short:\n",
        "                                    abbreviations[short] = full_name\n",
        "        except Exception as e2:\n",
        "            print(f\"Alternative loading also failed: {e2}\")\n",
        "\n",
        "    load_time = time.time() - start_time\n",
        "    print(f\" –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(abbreviations)} —Å–æ–∫—Ä–∞—â–µ–Ω–∏–π –∑–∞ {load_time:.2f} —Å–µ–∫—É–Ω–¥\")\n",
        "    return abbreviations\n",
        "\n",
        "# –§—É–Ω–∫—Ü–∏—è –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ —Ä–µ–≥—É–ª—è—Ä–Ω—ã—Ö –≤—ã—Ä–∞–∂–µ–Ω–∏–π\n",
        "def create_abbreviation_patterns(abbreviations_dict):\n",
        "    \"\"\"\n",
        "    –°–æ–∑–¥–∞–µ—Ç –ø–∞—Ç—Ç–µ—Ä–Ω—ã —Ä–µ–≥—É–ª—è—Ä–Ω—ã—Ö –≤—ã—Ä–∞–∂–µ–Ω–∏–π –¥–ª—è –≤—Å–µ—Ö —Å–æ–∫—Ä–∞—â–µ–Ω–∏–π\n",
        "    \"\"\"\n",
        "    print(f\"üîß –°–æ–∑–¥–∞–Ω–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –¥–ª—è {len(abbreviations_dict)} —Å–æ–∫—Ä–∞—â–µ–Ω–∏–π...\")\n",
        "    patterns = {}\n",
        "\n",
        "    for short_form, full_name in abbreviations_dict.items():\n",
        "        if '.' in short_form:\n",
        "            # –î–ª—è —Å–æ–∫—Ä–∞—â–µ–Ω–∏–π —Å —Ç–æ—á–∫–æ–π: –∏—â–µ–º —Å —Ç–æ—á–∫–æ–π, –ø–æ—Å–ª–µ –∫–æ—Ç–æ—Ä–æ–π –º–æ–∂–µ—Ç –±—ã—Ç—å –ø—Ä–æ–±–µ–ª –∏–ª–∏ –∫–æ–Ω–µ—Ü —Å—Ç—Ä–æ–∫–∏\n",
        "            pattern = r'\\b' + re.escape(short_form) + r'(?=\\s|$|\\.|,|;|\\)|\\]|\\})'\n",
        "        else:\n",
        "            # –î–ª—è —Å–æ–∫—Ä–∞—â–µ–Ω–∏–π –±–µ–∑ —Ç–æ—á–∫–∏: –æ–±—ã—á–Ω—ã–π –ø–æ–∏—Å–∫ —Å –≥—Ä–∞–Ω–∏—Ü–∞–º–∏ —Å–ª–æ–≤\n",
        "            pattern = r'\\b' + re.escape(short_form) + r'\\b'\n",
        "        patterns[short_form] = (pattern, full_name)\n",
        "\n",
        "    print(f\" –ü–∞—Ç—Ç–µ—Ä–Ω—ã —Å–æ–∑–¥–∞–Ω—ã –¥–ª—è {len(patterns)} —Å–æ–∫—Ä–∞—â–µ–Ω–∏–π\")\n",
        "    return patterns\n",
        "\n",
        "# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–æ–∏—Å–∫–∞ —Å–æ–∫—Ä–∞—â–µ–Ω–∏–π –≤ –æ–∫—Ä–µ—Å—Ç–Ω–æ—Å—Ç–∏ —Ç–æ–ø–æ–Ω–∏–º–∞\n",
        "def find_nearby_abbreviations(text, toponym, patterns, distance_words=2):\n",
        "    \"\"\"\n",
        "    –ò—â–µ—Ç —Å–æ–∫—Ä–∞—â–µ–Ω–∏—è –≤ –æ–∫—Ä–µ—Å—Ç–Ω–æ—Å—Ç–∏ —Ç–æ–ø–æ–Ω–∏–º–∞ –∏—Å–ø–æ–ª—å–∑—É—è –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ —Å–æ–∑–¥–∞–Ω–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã\n",
        "    \"\"\"\n",
        "    search_start_time = time.time()\n",
        "    nearby_abbreviations = []\n",
        "\n",
        "    # –ù–∞—Ö–æ–¥–∏–º –≤—Å–µ –≤—Ö–æ–∂–¥–µ–Ω–∏—è —Ç–æ–ø–æ–Ω–∏–º–∞ –≤ —Ç–µ–∫—Å—Ç–µ\n",
        "    toponym_search_start = time.time()\n",
        "    toponym_pattern = re.escape(toponym)\n",
        "    toponym_matches = list(re.finditer(toponym_pattern, text))\n",
        "    toponym_search_time = time.time() - toponym_search_start\n",
        "    print(f\"    –ü–æ–∏—Å–∫ —Ç–æ–ø–æ–Ω–∏–º–∞ '{toponym}': {toponym_search_time:.4f} —Å–µ–∫, –Ω–∞–π–¥–µ–Ω–æ {len(toponym_matches)} –≤—Ö–æ–∂–¥–µ–Ω–∏–π\")\n",
        "\n",
        "    for toponym_match in toponym_matches:\n",
        "        toponym_start = toponym_match.start()\n",
        "        toponym_end = toponym_match.end()\n",
        "\n",
        "        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –æ–∫—Ä–µ—Å—Ç–Ω–æ—Å—Ç—å –≤–æ–∫—Ä—É–≥ —Ç–æ–ø–æ–Ω–∏–º–∞ (–≤ —Å–∏–º–≤–æ–ª–∞—Ö)\n",
        "        # –ü—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω–æ: —Å—á–∏—Ç–∞–µ–º —á—Ç–æ –≤ —Å—Ä–µ–¥–Ω–µ–º —Å–ª–æ–≤–æ = 6 —Å–∏–º–≤–æ–ª–æ–≤ + –ø—Ä–æ–±–µ–ª\n",
        "        approx_distance_chars = distance_words * 7\n",
        "\n",
        "        search_start = max(0, toponym_start - approx_distance_chars)\n",
        "        search_end = min(len(text), toponym_end + approx_distance_chars)\n",
        "\n",
        "        search_area = text[search_start:search_end]\n",
        "\n",
        "        # –ò—â–µ–º –≤—Å–µ —Å–æ–∫—Ä–∞—â–µ–Ω–∏—è –≤ —ç—Ç–æ–π –æ–±–ª–∞—Å—Ç–∏\n",
        "        abbreviation_search_start = time.time()\n",
        "        for short_form, (pattern, full_name) in patterns.items():\n",
        "            abbreviation_matches = list(re.finditer(pattern, search_area))\n",
        "\n",
        "            for abbr_match in abbreviation_matches:\n",
        "                # –í—ã—á–∏—Å–ª—è–µ–º –∞–±—Å–æ–ª—é—Ç–Ω—É—é –ø–æ–∑–∏—Ü–∏—é —Å–æ–∫—Ä–∞—â–µ–Ω–∏—è –≤ —Ç–µ–∫—Å—Ç–µ\n",
        "                abbr_absolute_pos = search_start + abbr_match.start()\n",
        "\n",
        "                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ —Å–æ–∫—Ä–∞—â–µ–Ω–∏–µ –Ω–µ —è–≤–ª—è–µ—Ç—Å—è —á–∞—Å—Ç—å—é —Å–∞–º–æ–≥–æ —Ç–æ–ø–æ–Ω–∏–º–∞\n",
        "                if not (toponym_start <= abbr_absolute_pos <= toponym_end):\n",
        "                    abbr_info = {\n",
        "                        'abbreviation': abbr_match.group(),\n",
        "                        'full_name': full_name,\n",
        "                        'position': abbr_absolute_pos,\n",
        "                        'context': search_area\n",
        "                    }\n",
        "\n",
        "                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –¥—É–±–ª–∏–∫–∞—Ç—ã\n",
        "                    is_duplicate = False\n",
        "                    for existing in nearby_abbreviations:\n",
        "                        if (existing['abbreviation'] == abbr_info['abbreviation'] and\n",
        "                            existing['full_name'] == abbr_info['full_name'] and\n",
        "                            abs(existing['position'] - abbr_info['position']) < 10):\n",
        "                            is_duplicate = True\n",
        "                            break\n",
        "\n",
        "                    if not is_duplicate:\n",
        "                        nearby_abbreviations.append(abbr_info)\n",
        "\n",
        "        abbreviation_search_time = time.time() - abbreviation_search_start\n",
        "        print(f\"    –ü–æ–∏—Å–∫ —Å–æ–∫—Ä–∞—â–µ–Ω–∏–π –≤–æ–∫—Ä—É–≥ '{toponym}': {abbreviation_search_time:.4f} —Å–µ–∫\")\n",
        "\n",
        "    total_search_time = time.time() - search_start_time\n",
        "    if nearby_abbreviations:\n",
        "        print(f\"    –ù–∞–π–¥–µ–Ω–æ {len(nearby_abbreviations)} —Å–æ–∫—Ä–∞—â–µ–Ω–∏–π –∑–∞ {total_search_time:.4f} —Å–µ–∫\")\n",
        "    else:\n",
        "        print(f\"    –°–æ–∫—Ä–∞—â–µ–Ω–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –∑–∞ {total_search_time:.4f} —Å–µ–∫\")\n",
        "\n",
        "    return nearby_abbreviations\n",
        "\n",
        "def main(max_lines):\n",
        "    \"\"\"\n",
        "    –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Å—Ç—Ä–æ–∫\n",
        "\n",
        "    Args:\n",
        "        max_lines (int): –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–æ–∫ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏. –ï—Å–ª–∏ None - –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –≤—Å–µ.\n",
        "    \"\"\"\n",
        "    total_start_time = time.time()\n",
        "\n",
        "\n",
        "    # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å–æ–∫—Ä–∞—â–µ–Ω–∏—è\n",
        "    abbreviations_dict = load_abbreviations('abbreviations.csv')\n",
        "\n",
        "    # –°–æ–∑–¥–∞–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã —Ä–µ–≥—É–ª—è—Ä–Ω—ã—Ö –≤—ã—Ä–∞–∂–µ–Ω–∏–π –æ–¥–∏–Ω —Ä–∞–∑ –¥–ª—è –≤—Å–µ—Ö –≤—ã–∑–æ–≤–æ–≤\n",
        "    patterns_creation_start = time.time()\n",
        "    patterns = create_abbreviation_patterns(abbreviations_dict)\n",
        "    patterns_creation_time = time.time() - patterns_creation_start\n",
        "    print(f\"‚è± –û–±—â–µ–µ –≤—Ä–µ–º—è —Å–æ–∑–¥–∞–Ω–∏—è –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤: {patterns_creation_time:.4f} —Å–µ–∫\")\n",
        "\n",
        "    lines = df['main_text'].tolist()\n",
        "    print(f\"\\n –í—Å–µ–≥–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –≤ –¥–∞–Ω–Ω—ã—Ö: {len(lines)}\")\n",
        "\n",
        "    # –ü—Ä–∏–º–µ–Ω—è–µ–º –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É —Å—Ç—Ä–æ–∫\n",
        "    if max_lines is not None:\n",
        "        lines = lines[:max_lines]\n",
        "        print(f\" –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã—Ö {max_lines} —Å—Ç—Ä–æ–∫\")\n",
        "\n",
        "    df['main_text'] = df['main_text'].replace({float('nan'): \"\"})\n",
        "\n",
        "    num = 0\n",
        "    output_csv = 'output.csv'\n",
        "\n",
        "    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞\n",
        "    total_toponyms = 0\n",
        "    total_abbreviations = 0\n",
        "    processed_sentences = 0\n",
        "\n",
        "    # –í—Ä–µ–º—è –¥–ª—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏\n",
        "    total_toponym_time = 0\n",
        "    total_abbreviation_time = 0\n",
        "    total_nlp_time = 0\n",
        "\n",
        "    print(f\"\\n –ù–∞—á–∞–ª–æ –æ–±—Ä–∞–±–æ—Ç–∫–∏ {len(lines)} –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π...\")\n",
        "    processing_start_time = time.time()\n",
        "\n",
        "    with open(output_csv, 'w', encoding='utf-8') as outfile:\n",
        "        outfile.writelines(f\"sentence_id; toponyms_list; nearby_abbreviations \\n\")\n",
        "\n",
        "        # –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø: –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –±–∞—Ç—á–∞–º–∏ –¥–ª—è –ª—É—á—à–µ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏\n",
        "        batch_size = 100  # –ú–æ–∂–Ω–æ –Ω–∞—Å—Ç—Ä–æ–∏—Ç—å –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –æ–±—ä–µ–º–∞ –ø–∞–º—è—Ç–∏\n",
        "\n",
        "        for i in range(0, len(lines), batch_size):\n",
        "            batch_lines = lines[i:i + batch_size]\n",
        "            batch_indices = list(range(i, min(i + batch_size, len(lines))))\n",
        "\n",
        "            print(f\"\\n--- –û–±—Ä–∞–±–æ—Ç–∫–∞ –±–∞—Ç—á–∞ {i//batch_size + 1}/{(len(lines)-1)//batch_size + 1} ---\")\n",
        "\n",
        "            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –±–∞—Ç—á –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π\n",
        "            nlp_start_time = time.time()\n",
        "            batch_docs = []\n",
        "            for sentence in batch_lines:\n",
        "                if pd.isna(sentence) or sentence == \"\":\n",
        "                    batch_docs.append(None)\n",
        "                else:\n",
        "                    batch_docs.append(nlp(sentence))\n",
        "            batch_nlp_time = time.time() - nlp_start_time\n",
        "            total_nlp_time += batch_nlp_time\n",
        "            print(f\"‚è± –í—Ä–µ–º—è NLP –æ–±—Ä–∞–±–æ—Ç–∫–∏ –±–∞—Ç—á–∞: {batch_nlp_time:.4f} —Å–µ–∫\")\n",
        "\n",
        "            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥—ã–π –¥–æ–∫—É–º–µ–Ω—Ç –≤ –±–∞—Ç—á–µ\n",
        "            for j, (doc, sentence_idx) in enumerate(zip(batch_docs, batch_indices)):\n",
        "                sentence = batch_lines[j]\n",
        "\n",
        "                if doc is None:\n",
        "                    # –ü—É—Å—Ç–∞—è —Å—Ç—Ä–æ–∫–∞\n",
        "                    strk = f\"{sentence_idx}; ; \\n\"\n",
        "                    outfile.writelines(strk)\n",
        "                    num += 1\n",
        "                    continue\n",
        "\n",
        "                sentence_start_time = time.time()\n",
        "\n",
        "                print(f'\\n--- –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ {sentence_idx} ---')\n",
        "                print(f'–¢–µ–∫—Å—Ç: {sentence[:100]}...' if len(sentence) > 100 else f'–¢–µ–∫—Å—Ç: {sentence}')\n",
        "\n",
        "                sentence_toponyms = []\n",
        "                all_nearby_abbreviations = []\n",
        "\n",
        "                # –ü–æ–∏—Å–∫ —Ç–æ–ø–æ–Ω–∏–º–æ–≤ –≤ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω–æ–º –¥–æ–∫—É–º–µ–Ω—Ç–µ\n",
        "                toponym_search_start = time.time()\n",
        "                for entity in doc.ents:\n",
        "                    if entity.type == \"LOC\":\n",
        "                        entity_find_time = time.time() - toponym_search_start\n",
        "                        print(f'üìç –¢–æ–ø–æ–Ω–∏–º: {entity.text} (–Ω–∞–π–¥–µ–Ω –∑–∞ {entity_find_time:.4f} —Å–µ–∫)')\n",
        "                        sentence_toponyms.append(entity.text)\n",
        "                        total_toponyms += 1\n",
        "\n",
        "                        # –û–±–Ω–æ–≤–ª—è–µ–º –≤—Ä–µ–º—è –¥–ª—è —Å–ª–µ–¥—É—é—â–µ–≥–æ —Ç–æ–ø–æ–Ω–∏–º–∞\n",
        "                        toponym_search_start = time.time()\n",
        "\n",
        "                        # –ò—â–µ–º —Å–æ–∫—Ä–∞—â–µ–Ω–∏—è –≤ –æ–∫—Ä–µ—Å—Ç–Ω–æ—Å—Ç–∏ —ç—Ç–æ–≥–æ —Ç–æ–ø–æ–Ω–∏–º–∞\n",
        "                        abbreviation_search_start = time.time()\n",
        "                        nearby_abbr = find_nearby_abbreviations_advanced(\n",
        "                            sentence,\n",
        "                            entity.text,\n",
        "                            patterns,\n",
        "                            distance_words=5\n",
        "                        )\n",
        "                        abbreviation_time = time.time() - abbreviation_search_start\n",
        "                        total_abbreviation_time += abbreviation_time\n",
        "\n",
        "                        print(f\"    –û–±—â–µ–µ –≤—Ä–µ–º—è –ø–æ–∏—Å–∫–∞ —Å–æ–∫—Ä–∞—â–µ–Ω–∏–π –¥–ª—è '{entity.text}': {abbreviation_time:.4f} —Å–µ–∫\")\n",
        "\n",
        "                        for abbr_info in nearby_abbr:\n",
        "                            match_str = f\"{abbr_info['abbreviation']}\"\n",
        "                            if match_str not in all_nearby_abbreviations:\n",
        "                                all_nearby_abbreviations.append(match_str)\n",
        "                                total_abbreviations += 1\n",
        "                                print(f\"    –ù–∞–π–¥–µ–Ω–æ —Å–æ–∫—Ä–∞—â–µ–Ω–∏–µ: '{abbr_info['abbreviation']}' -> '{abbr_info['full_name']}'\")\n",
        "\n",
        "                # –ï—Å–ª–∏ –Ω–µ –±—ã–ª–æ –Ω–∞–π–¥–µ–Ω–æ —Ç–æ–ø–æ–Ω–∏–º–æ–≤, –≤—ã–≤–æ–¥–∏–º –≤—Ä–µ–º—è –ø–æ–∏—Å–∫–∞\n",
        "                if not sentence_toponyms:\n",
        "                    toponym_time = time.time() - toponym_search_start\n",
        "                    total_toponym_time += toponym_time\n",
        "                    print(f\" –í—Ä–µ–º—è –ø–æ–∏—Å–∫–∞ —Ç–æ–ø–æ–Ω–∏–º–æ–≤: {toponym_time:.4f} —Å–µ–∫ (–Ω–µ –Ω–∞–π–¥–µ–Ω–æ)\")\n",
        "\n",
        "                # –§–æ—Ä–º–∏—Ä—É–µ–º —Å—Ç—Ä–æ–∫—É –¥–ª—è –≤—ã–≤–æ–¥–∞\n",
        "                toponyms_str = ', '.join(sentence_toponyms)\n",
        "                abbreviations_str = ', '.join(all_nearby_abbreviations)\n",
        "\n",
        "                strk = f\"{sentence_idx}; {toponyms_str}; {abbreviations_str} \\n\"\n",
        "                outfile.writelines(strk)\n",
        "\n",
        "                sentence_time = time.time() - sentence_start_time\n",
        "                print(f\" –û–±—â–µ–µ –≤—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è: {sentence_time:.2f} —Å–µ–∫\")\n",
        "\n",
        "                processed_sentences += 1\n",
        "                num += 1\n",
        "\n",
        "            # –ü—Ä–æ–≥—Ä–µ—Å—Å –ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–≥–æ –±–∞—Ç—á–∞\n",
        "            elapsed = time.time() - processing_start_time\n",
        "            print(f\"\\n –ü—Ä–æ–≥—Ä–µ—Å—Å: –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ {processed_sentences}/{len(lines)} –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π\")\n",
        "            print(f\" –ü—Ä–æ—à–ª–æ –≤—Ä–µ–º–µ–Ω–∏: {timedelta(seconds=int(elapsed))}\")\n",
        "\n",
        "            # –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –ø–æ —ç—Ç–∞–ø–∞–º\n",
        "            if processed_sentences > 0:\n",
        "                avg_nlp_time = total_nlp_time / processed_sentences\n",
        "                if total_toponyms > 0:\n",
        "                    avg_abbr_time = total_abbreviation_time / total_toponyms\n",
        "                    print(f\" –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –ø–æ–∏—Å–∫–∞ —Å–æ–∫—Ä–∞—â–µ–Ω–∏–π –Ω–∞ —Ç–æ–ø–æ–Ω–∏–º: {avg_abbr_time:.4f} —Å–µ–∫\")\n",
        "\n",
        "                print(f\" –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è NLP: {avg_nlp_time:.4f} —Å–µ–∫\")\n",
        "\n",
        "                avg_time = elapsed / processed_sentences\n",
        "                remaining = (len(lines) - processed_sentences) * avg_time\n",
        "                print(f\" –û—Å—Ç–∞–≤—à–µ–µ—Å—è –≤—Ä–µ–º—è: ~{timedelta(seconds=int(remaining))}\")\n",
        "\n",
        "    total_time = time.time() - total_start_time\n",
        "\n",
        "    # –§–∏–Ω–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Å –¥–µ—Ç–∞–ª–∏–∑–∞—Ü–∏–µ–π –ø–æ –≤—Ä–µ–º–µ–Ω–∏\n",
        "    print(f\"\\n –û–ë–†–ê–ë–û–¢–ö–ê –ó–ê–í–ï–†–®–ï–ù–ê!\")\n",
        "    print(f\" –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:\")\n",
        "    print(f\"   - –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π: {processed_sentences}\")\n",
        "    print(f\"   - –ù–∞–π–¥–µ–Ω–æ —Ç–æ–ø–æ–Ω–∏–º–æ–≤: {total_toponyms}\")\n",
        "    print(f\"   - –ù–∞–π–¥–µ–Ω–æ —Å–æ–∫—Ä–∞—â–µ–Ω–∏–π: {total_abbreviations}\")\n",
        "    print(f\"   - –û–±—â–µ–µ –≤—Ä–µ–º—è: {timedelta(seconds=int(total_time))}\")\n",
        "\n",
        "    if processed_sentences > 0:\n",
        "        print(f\"\\n‚è± –î–µ—Ç–∞–ª–∏–∑–∞—Ü–∏—è –≤—Ä–µ–º–µ–Ω–∏:\")\n",
        "        print(f\"   - –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ: {total_time/processed_sentences:.2f} —Å–µ–∫\")\n",
        "        print(f\"   - –û–±—â–µ–µ –≤—Ä–µ–º—è NLP: {timedelta(seconds=int(total_nlp_time))}\")\n",
        "        print(f\"   - –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è NLP –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ: {total_nlp_time/processed_sentences:.4f} —Å–µ–∫\")\n",
        "        print(f\"   - –í—Ä–µ–º—è —Å–æ–∑–¥–∞–Ω–∏—è –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤: {patterns_creation_time:.4f} —Å–µ–∫\")\n",
        "\n",
        "        if total_toponyms > 0:\n",
        "            print(f\"   - –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –ø–æ–∏—Å–∫–∞ —Å–æ–∫—Ä–∞—â–µ–Ω–∏–π –Ω–∞ —Ç–æ–ø–æ–Ω–∏–º: {total_abbreviation_time/total_toponyms:.4f} —Å–µ–∫\")\n",
        "            print(f\"   - –°—Ä–µ–¥–Ω–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–ø–æ–Ω–∏–º–æ–≤ –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ: {total_toponyms/processed_sentences:.2f}\")\n",
        "            print(f\"   - –°—Ä–µ–¥–Ω–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–æ–∫—Ä–∞—â–µ–Ω–∏–π –Ω–∞ —Ç–æ–ø–æ–Ω–∏–º: {total_abbreviations/max(total_toponyms, 1):.2f}\")\n",
        "\n",
        "    print(f\"   - –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {output_csv}\")\n",
        "# –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω–∞—è –≤–µ—Ä—Å–∏—è —Å –±–æ–ª–µ–µ —Ç–æ—á–Ω—ã–º –ø–æ–∏—Å–∫–æ–º –ø–æ —Å–ª–æ–≤–∞–º\n",
        "def find_nearby_abbreviations_advanced(text, toponym, patterns, distance_words=2):\n",
        "    \"\"\"\n",
        "    –£–ª—É—á—à–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è –ø–æ–∏—Å–∫–∞ —Å —É—á–µ—Ç–æ–º —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è –≤ —Å–ª–æ–≤–∞—Ö\n",
        "    \"\"\"\n",
        "    search_start_time = time.time()\n",
        "    nearby_abbreviations = []\n",
        "\n",
        "    # –¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –ø–æ–∑–∏—Ü–∏–π\n",
        "    tokenization_start = time.time()\n",
        "    words_with_positions = []\n",
        "    pattern = r'\\b\\w+(?:\\.\\w+)*\\b|[^\\w\\s]'\n",
        "    for match in re.finditer(pattern, text):\n",
        "        word = match.group()\n",
        "        if word.strip():  # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º —á–∏—Å—Ç—ã–µ –ø—Ä–æ–±–µ–ª—ã\n",
        "            words_with_positions.append({\n",
        "                'word': word,\n",
        "                'start': match.start(),\n",
        "                'end': match.end()\n",
        "            })\n",
        "    tokenization_time = time.time() - tokenization_start\n",
        "    print(f\"    –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞: {tokenization_time:.4f} —Å–µ–∫\")\n",
        "\n",
        "    # –ù–∞—Ö–æ–¥–∏–º –ø–æ–∑–∏—Ü–∏–∏ —Ç–æ–ø–æ–Ω–∏–º–∞\n",
        "    toponym_search_start = time.time()\n",
        "    toponym_positions = []\n",
        "    for i, word_info in enumerate(words_with_positions):\n",
        "        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ—Ç–¥–µ–ª—å–Ω—ã–µ —Å–ª–æ–≤–∞ –∏ –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏ —Å–ª–æ–≤ –¥–ª—è —Ç–æ–ø–æ–Ω–∏–º–∞\n",
        "        if word_info['word'] in toponym:\n",
        "            # –ü—Ä–æ—Å—Ç–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ - –º–æ–∂–Ω–æ —É–ª—É—á—à–∏—Ç—å –¥–ª—è –º–Ω–æ–≥–æ—Å–ª–æ–≤–Ω—ã—Ö —Ç–æ–ø–æ–Ω–∏–º–æ–≤\n",
        "            toponym_positions.append(i)\n",
        "    toponym_search_time = time.time() - toponym_search_start\n",
        "    print(f\"    –ü–æ–∏—Å–∫ –ø–æ–∑–∏—Ü–∏–π —Ç–æ–ø–æ–Ω–∏–º–∞: {toponym_search_time:.4f} —Å–µ–∫, –Ω–∞–π–¥–µ–Ω–æ {len(toponym_positions)} –ø–æ–∑–∏—Ü–∏–π\")\n",
        "\n",
        "    # –î–ª—è –∫–∞–∂–¥–æ–π –ø–æ–∑–∏—Ü–∏–∏ —Ç–æ–ø–æ–Ω–∏–º–∞ –∏—â–µ–º —Å–æ—Å–µ–¥–Ω–∏–µ —Å–æ–∫—Ä–∞—â–µ–Ω–∏—è\n",
        "    abbreviation_search_start = time.time()\n",
        "    for pos in toponym_positions:\n",
        "        start_idx = max(0, pos - distance_words)\n",
        "        end_idx = min(len(words_with_positions), pos + distance_words + 1)\n",
        "\n",
        "        for i in range(start_idx, end_idx):\n",
        "            word_info = words_with_positions[i]\n",
        "            word = word_info['word']\n",
        "\n",
        "            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Å–ª–æ–≤–æ —Å–æ–∫—Ä–∞—â–µ–Ω–∏–µ–º\n",
        "            for short_form, (pattern, full_name) in patterns.items():\n",
        "                # –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º —Å –æ—á–∏—â–µ–Ω–Ω–æ–π –≤–µ—Ä—Å–∏–µ–π —Å–æ–∫—Ä–∞—â–µ–Ω–∏—è\n",
        "                clean_short = short_form.replace('.', '')\n",
        "                clean_word = word.replace('.', '')\n",
        "\n",
        "                if clean_word == clean_short:\n",
        "                    abbr_info = {\n",
        "                        'abbreviation': short_form,\n",
        "                        'full_name': full_name,\n",
        "                        'position': word_info['start']\n",
        "                    }\n",
        "\n",
        "                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –¥—É–±–ª–∏–∫–∞—Ç—ã\n",
        "                    is_duplicate = False\n",
        "                    for existing in nearby_abbreviations:\n",
        "                        if (existing['abbreviation'] == abbr_info['abbreviation'] and\n",
        "                            existing['full_name'] == abbr_info['full_name']):\n",
        "                            is_duplicate = True\n",
        "                            break\n",
        "\n",
        "                    if not is_duplicate:\n",
        "                        nearby_abbreviations.append(abbr_info)\n",
        "\n",
        "    abbreviation_search_time = time.time() - abbreviation_search_start\n",
        "    total_search_time = time.time() - search_start_time\n",
        "\n",
        "    print(f\"    –ü–æ–∏—Å–∫ —Å–æ–∫—Ä–∞—â–µ–Ω–∏–π: {abbreviation_search_time:.4f} —Å–µ–∫\")\n",
        "    if nearby_abbreviations:\n",
        "        print(f\"    –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –ø–æ–∏—Å–∫: –Ω–∞–π–¥–µ–Ω–æ {len(nearby_abbreviations)} —Å–æ–∫—Ä–∞—â–µ–Ω–∏–π –∑–∞ {total_search_time:.4f} —Å–µ–∫\")\n",
        "    else:\n",
        "        print(f\"    –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –ø–æ–∏—Å–∫: —Å–æ–∫—Ä–∞—â–µ–Ω–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –∑–∞ {total_search_time:.4f} —Å–µ–∫\")\n",
        "\n",
        "    return nearby_abbreviations\n",
        "\n",
        "# –ó–∞–ø—É—Å–∫ –æ—Å–Ω–æ–≤–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–∏\n",
        "if __name__ == \"__main__\":\n",
        "    # –û–±—Ä–∞–±–æ—Ç–∞—Ç—å —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ 50 —Å—Ç—Ä–æ–∫ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è\n",
        "    #main(max_lines=100)\n",
        "\n",
        "    # –ò–ª–∏ –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å –≤—Å–µ —Å—Ç—Ä–æ–∫–∏:\n",
        "    main(None)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPaz2yzE/bOfoqWPz2J4U5e",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}