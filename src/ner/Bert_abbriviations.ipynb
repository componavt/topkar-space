{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPxdtA6c/hfU2XpB+ZJqWpa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c42bbc3f77be45009c021f37b55469f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2cc7c759dce44826bb2deae9ba6e4291",
              "IPY_MODEL_4f24e19829704d118bbba90ba4a9d9dd",
              "IPY_MODEL_c3bf29ec555e49ecbb344706c5d0219e"
            ],
            "layout": "IPY_MODEL_d31a7498916f43958a542edb3d48b04b"
          }
        },
        "2cc7c759dce44826bb2deae9ba6e4291": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9f81448da29c493faffb6f01653e7a57",
            "placeholder": "​",
            "style": "IPY_MODEL_4af55b527b754eb8bcc2a0ab5405750b",
            "value": "Downloading builder script: "
          }
        },
        "4f24e19829704d118bbba90ba4a9d9dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e7b4eb15c2664114ada1536896ed641d",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2d621f733d944f9d8ff0a953beef46f7",
            "value": 1
          }
        },
        "c3bf29ec555e49ecbb344706c5d0219e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3a3142c7b96f44c690ced214e47cdd3e",
            "placeholder": "​",
            "style": "IPY_MODEL_7f4c0a9e046748b88b394516cc1dd28d",
            "value": " 4.20k/? [00:00&lt;00:00, 377kB/s]"
          }
        },
        "d31a7498916f43958a542edb3d48b04b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9f81448da29c493faffb6f01653e7a57": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4af55b527b754eb8bcc2a0ab5405750b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e7b4eb15c2664114ada1536896ed641d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "2d621f733d944f9d8ff0a953beef46f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3a3142c7b96f44c690ced214e47cdd3e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7f4c0a9e046748b88b394516cc1dd28d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0df4ee3ce7f2479f904b1741b8b5c5f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_94adb047d70d42d6a27a17cec08ef45f",
              "IPY_MODEL_2b30ede3492942fa9456f5f6732c7e15",
              "IPY_MODEL_f4774f7c3be74ae383ce81f2fcdcf0c3"
            ],
            "layout": "IPY_MODEL_5ce53bfa7f8c4c33a079491f1c2b898e"
          }
        },
        "94adb047d70d42d6a27a17cec08ef45f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3e7aaea13a364ef786afe4965df59046",
            "placeholder": "​",
            "style": "IPY_MODEL_4faad44d87cc454390fc14eb7c5e9bab",
            "value": "Downloading builder script: "
          }
        },
        "2b30ede3492942fa9456f5f6732c7e15": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_47b7af2a0e5e431d86504146b940f820",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_79399796517d4a588063ab5bb2f5b61c",
            "value": 1
          }
        },
        "f4774f7c3be74ae383ce81f2fcdcf0c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0b37c138c816462d86a8c0442d28a679",
            "placeholder": "​",
            "style": "IPY_MODEL_9523460ba94d4ee699acd72d2609c046",
            "value": " 6.79k/? [00:00&lt;00:00, 402kB/s]"
          }
        },
        "5ce53bfa7f8c4c33a079491f1c2b898e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3e7aaea13a364ef786afe4965df59046": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4faad44d87cc454390fc14eb7c5e9bab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "47b7af2a0e5e431d86504146b940f820": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "79399796517d4a588063ab5bb2f5b61c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0b37c138c816462d86a8c0442d28a679": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9523460ba94d4ee699acd72d2609c046": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/componavt/topkar-space/blob/main/src/ner/Bert_abbriviations.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Упрщённая версия только для географических аббривиатур"
      ],
      "metadata": {
        "id": "EMAsXhXxwojH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import pickle\n",
        "\n",
        "class GeographyAbbreviationModel:\n",
        "    \"\"\"Упрощенная модель для географических аббревиатур\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.tokenizer = None\n",
        "        self.model = None\n",
        "        self.label_encoder = None\n",
        "\n",
        "    def create_training_data(self):\n",
        "        \"\"\"Создает пример данных для географических объектов\"\"\"\n",
        "\n",
        "        data = [\n",
        "            # Города с разными префиксами\n",
        "            {\"text\": \"Я живу в г. Москва.\", \"abbreviation\": \"г.\", \"expansion\": \"город\"},\n",
        "            {\"text\": \"Поеду в г. Санкт-Петербург.\", \"abbreviation\": \"г.\", \"expansion\": \"город\"},\n",
        "            {\"text\": \"р. Волга красива.\", \"abbreviation\": \"р.\", \"expansion\": \"река\"},\n",
        "            {\"text\": \"оз. Байкал глубокое.\", \"abbreviation\": \"оз.\", \"expansion\": \"озеро\"},\n",
        "\n",
        "            # Адреса\n",
        "            {\"text\": \"ул. Ленина центральная.\", \"abbreviation\": \"ул.\", \"expansion\": \"улица\"},\n",
        "            {\"text\": \"пр. Мира широкий.\", \"abbreviation\": \"пр.\", \"expansion\": \"проспект\"},\n",
        "            {\"text\": \"пл. Победы большая.\", \"abbreviation\": \"пл.\", \"expansion\": \"площадь\"},\n",
        "\n",
        "            # Страны и организации\n",
        "            {\"text\": \"США - большая страна.\", \"abbreviation\": \"США\", \"expansion\": \"Соединенные Штаты Америки\"},\n",
        "            {\"text\": \"РФ нашла решение.\", \"abbreviation\": \"РФ\", \"expansion\": \"Российская Федерация\"},\n",
        "            {\"text\": \"ЕС принял закон.\", \"abbreviation\": \"ЕС\", \"expansion\": \"Европейский Союз\"},\n",
        "        ]\n",
        "\n",
        "        df = pd.DataFrame(data)\n",
        "        df.to_csv('geo_abbreviations.csv', index=False, encoding='utf-8')\n",
        "        print(f\"Создан CSV с {len(df)} географическими примерами\")\n",
        "        return df\n",
        "\n",
        "    def prepare_dataset(self, csv_path='data_learn.csv'):\n",
        "        \"\"\"Подготавливает данные для обучения\"\"\"\n",
        "\n",
        "        try:\n",
        "            df = pd.read_csv(csv_path, encoding='utf-8')\n",
        "        except:\n",
        "            print(\"CSV не найден, создаю пример...\")\n",
        "            df = self.create_training_data()\n",
        "\n",
        "        # Простая подготовка текста\n",
        "        df['input_text'] = df.apply(\n",
        "            lambda row: f\"Текст: {row['text']} | Объект: {row['abbreviation']}\",\n",
        "            axis=1\n",
        "        )\n",
        "\n",
        "        # Кодируем метки\n",
        "        self.label_encoder = LabelEncoder()\n",
        "        df['label'] = self.label_encoder.fit_transform(df['expansion'])\n",
        "\n",
        "        print(f\"Классы: {list(self.label_encoder.classes_)}\")\n",
        "        return df\n",
        "\n",
        "    def train(self, df):\n",
        "        \"\"\"Обучает модель\"\"\"\n",
        "\n",
        "        # Токенайзер\n",
        "        self.tokenizer = BertTokenizer.from_pretrained('cointegrated/rubert-tiny2')\n",
        "\n",
        "        # Модель\n",
        "        self.model = BertForSequenceClassification.from_pretrained(\n",
        "            'cointegrated/rubert-tiny2',\n",
        "            num_labels=len(self.label_encoder.classes_)\n",
        "        )\n",
        "\n",
        "        # Простое обучение\n",
        "        from torch.utils.data import DataLoader, TensorDataset\n",
        "        import torch.optim as optim\n",
        "\n",
        "        # Подготовка данных\n",
        "        inputs = self.tokenizer(\n",
        "            df['input_text'].tolist(),\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            max_length=128,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        labels = torch.tensor(df['label'].tolist())\n",
        "\n",
        "        dataset = TensorDataset(inputs['input_ids'], inputs['attention_mask'], labels)\n",
        "        dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
        "\n",
        "        # Обучение\n",
        "        optimizer = optim.AdamW(self.model.parameters(), lr=2e-5)\n",
        "\n",
        "        self.model.train()\n",
        "        for epoch in range(3):  # 3 эпохи\n",
        "            total_loss = 0\n",
        "            for batch in dataloader:\n",
        "                input_ids, attention_mask, batch_labels = batch\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                outputs = self.model(\n",
        "                    input_ids=input_ids,\n",
        "                    attention_mask=attention_mask,\n",
        "                    labels=batch_labels\n",
        "                )\n",
        "\n",
        "                loss = outputs.loss\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                total_loss += loss.item()\n",
        "\n",
        "            print(f\"Эпоха {epoch+1}, Loss: {total_loss/len(dataloader):.4f}\")\n",
        "\n",
        "        # Сохраняем\n",
        "        self.model.save_pretrained('geo_model')\n",
        "        self.tokenizer.save_pretrained('geo_model')\n",
        "        with open('geo_label_encoder.pkl', 'wb') as f:\n",
        "            pickle.dump(self.label_encoder, f)\n",
        "\n",
        "        print(\"Модель сохранена\")\n",
        "\n",
        "    def predict(self, text, abbreviation):\n",
        "        \"\"\"Предсказывает географический объект\"\"\"\n",
        "\n",
        "        if self.model is None:\n",
        "            self.model = BertForSequenceClassification.from_pretrained('geo_model')\n",
        "            self.tokenizer = BertTokenizer.from_pretrained('geo_model')\n",
        "            with open('geo_label_encoder.pkl', 'rb') as f:\n",
        "                self.label_encoder = pickle.load(f)\n",
        "\n",
        "        input_text = f\"Текст: {text} | Объект: {abbreviation}\"\n",
        "\n",
        "        inputs = self.tokenizer(\n",
        "            input_text,\n",
        "            return_tensors='pt',\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            max_length=128\n",
        "        )\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(**inputs)\n",
        "            prediction = torch.argmax(outputs.logits, dim=1).item()\n",
        "\n",
        "        return self.label_encoder.inverse_transform([prediction])[0]\n",
        "\n",
        "# Использование\n",
        "if __name__ == \"__main__\":\n",
        "    model = GeographyAbbreviationModel()\n",
        "    df = model.prepare_dataset()\n",
        "    model.train(df)\n",
        "\n",
        "    # Тест\n",
        "    result = model.predict(\"Пожня на юго-западе в 300 м.\", \"м.\")\n",
        "    print(f\"Предсказание: {result}\")  # улица Ленина"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QsGE3v8XaEd7",
        "outputId": "0afc8611-fde5-4c55-f792-0beb17826915"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Классы: ['берег', 'болото', 'бор', 'деревня', 'лес', 'метр', 'мост', 'мостки', 'мыс', 'нива', 'озеро', 'река', 'урочище', 'фамилия']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at cointegrated/rubert-tiny2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Эпоха 1, Loss: 2.0770\n",
            "Эпоха 2, Loss: 1.6511\n",
            "Эпоха 3, Loss: 1.3784\n",
            "Модель сохранена\n",
            "Предсказание: деревня\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n",
        "!pip install evaluate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yMVpFt0IRnSx",
        "outputId": "93d3f1b5-9d42-4895-d72a-373e90ef0f37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets) (3.20.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.36.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets) (6.0.3)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.13.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2025.11.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.22.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Collecting evaluate\n",
            "  Downloading evaluate-0.4.6-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from evaluate) (4.0.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from evaluate) (2.0.2)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.12/dist-packages (from evaluate) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from evaluate) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.12/dist-packages (from evaluate) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.12/dist-packages (from evaluate) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from evaluate) (3.6.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.12/dist-packages (from evaluate) (0.70.16)\n",
            "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from evaluate) (0.36.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from evaluate) (25.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (3.20.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (18.1.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (6.0.3)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (3.13.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.7.0->evaluate) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (2025.11.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.22.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n",
            "Downloading evaluate-0.4.6-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: evaluate\n",
            "Successfully installed evaluate-0.4.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Тестовый вариант с более общим подходом"
      ],
      "metadata": {
        "id": "0woXqXm8wy64"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from transformers import (\n",
        "    BertTokenizer,\n",
        "    BertForSequenceClassification,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    EarlyStoppingCallback\n",
        ")\n",
        "from torch.utils.data import Dataset\n",
        "import evaluate\n",
        "import warnings\n",
        "import os\n",
        "import pickle\n",
        "from typing import Dict, List, Tuple, Optional, Any\n",
        "import logging\n",
        "from dataclasses import dataclass\n",
        "import math\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Настройка логирования\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# ==================== 1. КОНФИГУРАЦИЯ ====================\n",
        "\n",
        "@dataclass\n",
        "class Config:\n",
        "    \"\"\"Конфигурация обучения\"\"\"\n",
        "    MODEL_NAME: str = 'cointegrated/rubert-tiny2'\n",
        "    CSV_PATH: str = 'abbreviations_dataset.csv'\n",
        "    OUTPUT_DIR: str = './abbreviation_model'\n",
        "    MAX_LENGTH: int = 128  # Уменьшили для более быстрого обучения\n",
        "    BATCH_SIZE: int = 4    # Уменьшили для небольших данных\n",
        "    EPOCHS: int = 10       # Уменьшили количество эпох\n",
        "    TEST_SIZE: float = 0.3  # Увеличили тестовую выборку\n",
        "    MIN_SAMPLES_PER_CLASS: int = 2  # Минимум примеров на класс\n",
        "    RANDOM_SEED: int = 42\n",
        "    SPECIAL_TOKENS: Dict[str, str] = None\n",
        "\n",
        "    def __post_init__(self):\n",
        "        if self.SPECIAL_TOKENS is None:\n",
        "            self.SPECIAL_TOKENS = {\n",
        "                'start': '[ABBR]',\n",
        "                'end': '[/ABBR]',\n",
        "                'sep': '[SEP]'\n",
        "            }\n",
        "\n",
        "config = Config()\n",
        "\n",
        "# ==================== 2. УТИЛИТЫ ДЛЯ РАБОТЫ С АББРЕВИАТУРАМИ ====================\n",
        "\n",
        "class AbbreviationProcessor:\n",
        "    \"\"\"Класс для обработки аббревиатур в тексте\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def find_all_abbreviations(text: str, min_length: int = 2) -> List[Dict[str, Any]]:\n",
        "        \"\"\"Находит все возможные аббревиатуры в тексте.\"\"\"\n",
        "        abbreviations = []\n",
        "\n",
        "        pattern = r'\\b(?:[A-ZА-ЯЁ]{2,}|\\d+[A-ZА-ЯЁ]+|[A-ZА-ЯЁ]+\\d+)[A-ZА-ЯЁ\\d]*\\b'\n",
        "\n",
        "        for match in re.finditer(pattern, text):\n",
        "            abbr = match.group()\n",
        "            start, end = match.span()\n",
        "\n",
        "            if len(abbr) < min_length:\n",
        "                continue\n",
        "\n",
        "            abbreviations.append({\n",
        "                'text': abbr,\n",
        "                'start': start,\n",
        "                'end': end,\n",
        "                'context': text[max(0, start-30):min(len(text), end+30)]\n",
        "            })\n",
        "\n",
        "        return abbreviations\n",
        "\n",
        "    @staticmethod\n",
        "    def mark_specific_abbreviation(\n",
        "        text: str,\n",
        "        target_abbr: str,\n",
        "        target_start: Optional[int] = None\n",
        "    ) -> Tuple[str, int]:\n",
        "        \"\"\"Маркирует конкретную аббревиатуру в тексте специальными токенами.\"\"\"\n",
        "        if target_start is not None:\n",
        "            marked_text = (\n",
        "                text[:target_start] +\n",
        "                f\"{config.SPECIAL_TOKENS['start']}{target_abbr}{config.SPECIAL_TOKENS['end']}\" +\n",
        "                text[target_start + len(target_abbr):]\n",
        "            )\n",
        "            return marked_text, target_start\n",
        "\n",
        "        pattern = re.compile(r'\\b' + re.escape(target_abbr) + r'\\b', re.IGNORECASE)\n",
        "        matches = list(pattern.finditer(text))\n",
        "\n",
        "        if not matches:\n",
        "            marked_text = f\"{text} {config.SPECIAL_TOKENS['sep']} {target_abbr}\"\n",
        "            return marked_text, len(text) + len(config.SPECIAL_TOKENS['sep']) + 1\n",
        "\n",
        "        first_match = matches[0]\n",
        "        start, end = first_match.span()\n",
        "\n",
        "        marked_text = (\n",
        "            text[:start] +\n",
        "            f\"{config.SPECIAL_TOKENS['start']}{target_abbr}{config.SPECIAL_TOKENS['end']}\" +\n",
        "            text[end:]\n",
        "        )\n",
        "\n",
        "        return marked_text, start\n",
        "\n",
        "    @staticmethod\n",
        "    def prepare_training_example(\n",
        "        text: str,\n",
        "        abbreviation: str,\n",
        "        expansion: str,\n",
        "        position: Optional[int] = None\n",
        "    ) -> Dict[str, Any]:\n",
        "        \"\"\"Подготавливает один пример для обучения.\"\"\"\n",
        "        marked_text, abbr_start = AbbreviationProcessor.mark_specific_abbreviation(\n",
        "            text, abbreviation, position\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'original_text': text,\n",
        "            'marked_text': marked_text,\n",
        "            'abbreviation': abbreviation,\n",
        "            'expansion': expansion,\n",
        "            'abbreviation_start': abbr_start,\n",
        "        }\n",
        "\n",
        "# ==================== 3. ЧТЕНИЕ И ПОДГОТОВКА ДАННЫХ ====================\n",
        "\n",
        "def load_and_prepare_data(csv_path: str) -> Tuple[pd.DataFrame, LabelEncoder]:\n",
        "    \"\"\"Загружает и подготавливает данные из CSV файла.\"\"\"\n",
        "\n",
        "    logger.info(f\"Загрузка данных из {csv_path}\")\n",
        "\n",
        "    if not os.path.exists(csv_path):\n",
        "        logger.warning(f\"Файл не найден. Создаю пример данных...\")\n",
        "        return create_balanced_sample_dataset()\n",
        "\n",
        "    try:\n",
        "        # Пробуем разные кодировки\n",
        "        for encoding in ['utf-8', 'cp1251', 'latin1']:\n",
        "            try:\n",
        "                df = pd.read_csv(csv_path, encoding=encoding)\n",
        "                if len(df.columns) >= 3:\n",
        "                    logger.info(f\"Успешно загружен CSV с кодировкой '{encoding}'\")\n",
        "                    break\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "        required_columns = ['text', 'abbreviation', 'expansion']\n",
        "        missing_columns = [col for col in required_columns if col not in df.columns]\n",
        "\n",
        "        if missing_columns:\n",
        "            raise ValueError(f\"Отсутствуют обязательные колонки: {missing_columns}\")\n",
        "\n",
        "        initial_count = len(df)\n",
        "        df = df.dropna(subset=required_columns)\n",
        "\n",
        "        df['text'] = df['text'].astype(str).str.strip()\n",
        "        df['abbreviation'] = df['abbreviation'].astype(str).str.strip().str.upper()\n",
        "        df['expansion'] = df['expansion'].astype(str).str.strip()\n",
        "\n",
        "        df = df.drop_duplicates(subset=['text', 'abbreviation', 'expansion'])\n",
        "\n",
        "        logger.info(f\"Загружено {len(df)} примеров (удалено {initial_count - len(df)})\")\n",
        "\n",
        "        # Если слишком мало данных, используем пример\n",
        "        if len(df) < config.MIN_SAMPLES_PER_CLASS * 3:\n",
        "            logger.warning(f\"Слишком мало данных ({len(df)} примеров). Использую пример данных...\")\n",
        "            return create_balanced_sample_dataset()\n",
        "\n",
        "        processed_examples = []\n",
        "\n",
        "        for idx, row in df.iterrows():\n",
        "            position = None\n",
        "            if 'position' in row and pd.notna(row['position']):\n",
        "                try:\n",
        "                    position = int(row['position'])\n",
        "                except:\n",
        "                    position = None\n",
        "\n",
        "            example = AbbreviationProcessor.prepare_training_example(\n",
        "                text=row['text'],\n",
        "                abbreviation=row['abbreviation'],\n",
        "                expansion=row['expansion'],\n",
        "                position=position\n",
        "            )\n",
        "\n",
        "            if 'domain' in row:\n",
        "                example['domain'] = row['domain']\n",
        "\n",
        "            processed_examples.append(example)\n",
        "\n",
        "        processed_df = pd.DataFrame(processed_examples)\n",
        "\n",
        "        # Фильтруем классы с недостаточным количеством примеров\n",
        "        class_counts = processed_df['expansion'].value_counts()\n",
        "        valid_classes = class_counts[class_counts >= config.MIN_SAMPLES_PER_CLASS].index\n",
        "\n",
        "        if len(valid_classes) < 2:\n",
        "            logger.warning(f\"Достаточно данных только для {len(valid_classes)} классов. Использую пример данных...\")\n",
        "            return create_balanced_sample_dataset()\n",
        "\n",
        "        processed_df = processed_df[processed_df['expansion'].isin(valid_classes)]\n",
        "\n",
        "        label_encoder = LabelEncoder()\n",
        "        processed_df['label_id'] = label_encoder.fit_transform(processed_df['expansion'])\n",
        "\n",
        "        logger.info(f\"Уникальных аббревиатур: {processed_df['abbreviation'].nunique()}\")\n",
        "        logger.info(f\"Уникальных расшифровок: {len(label_encoder.classes_)}\")\n",
        "\n",
        "        logger.info(\"\\nРаспределение по классам:\")\n",
        "        for expansion, count in class_counts.items():\n",
        "            logger.info(f\"  {expansion[:40]:40} : {count:3}\")\n",
        "\n",
        "        return processed_df, label_encoder\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Ошибка при загрузке данных: {e}\")\n",
        "        logger.info(\"Использую пример данных...\")\n",
        "        return create_balanced_sample_dataset()\n",
        "\n",
        "def create_balanced_sample_dataset() -> Tuple[pd.DataFrame, LabelEncoder]:\n",
        "    \"\"\"Создает сбалансированный пример датасета.\"\"\"\n",
        "\n",
        "    logger.info(\"Создание сбалансированного примера данных...\")\n",
        "\n",
        "    # Создаем небольшой но сбалансированный датасет\n",
        "    examples = []\n",
        "\n",
        "    # Базовые аббревиатуры с несколькими примерами каждая\n",
        "    base_abbreviations = [\n",
        "        ('API', 'Application Programming Interface', 'IT'),\n",
        "        ('CPU', 'Central Processing Unit', 'Компьютеры'),\n",
        "        ('HTTP', 'Hypertext Transfer Protocol', 'IT'),\n",
        "        ('VPN', 'Virtual Private Network', 'IT'),\n",
        "    ]\n",
        "\n",
        "    # Создаем по 2 примера для каждой аббревиатуры\n",
        "    for abbr, expansion, domain in base_abbreviations:\n",
        "        examples.append({\n",
        "            'text': f'Используйте {abbr} для доступа к данным.',\n",
        "            'abbreviation': abbr,\n",
        "            'expansion': expansion,\n",
        "            'domain': domain\n",
        "        })\n",
        "        examples.append({\n",
        "            'text': f'Система работает через {abbr} протокол.',\n",
        "            'abbreviation': abbr,\n",
        "            'expansion': expansion,\n",
        "            'domain': domain\n",
        "        })\n",
        "\n",
        "    df = pd.DataFrame(examples)\n",
        "\n",
        "    processed_examples = []\n",
        "\n",
        "    for _, row in df.iterrows():\n",
        "        example = AbbreviationProcessor.prepare_training_example(\n",
        "            text=row['text'],\n",
        "            abbreviation=row['abbreviation'],\n",
        "            expansion=row['expansion']\n",
        "        )\n",
        "\n",
        "        if 'domain' in row:\n",
        "            example['domain'] = row['domain']\n",
        "\n",
        "        processed_examples.append(example)\n",
        "\n",
        "    processed_df = pd.DataFrame(processed_examples)\n",
        "\n",
        "    label_encoder = LabelEncoder()\n",
        "    processed_df['label_id'] = label_encoder.fit_transform(processed_df['expansion'])\n",
        "\n",
        "    logger.info(f\"Создано {len(processed_df)} примеров для {len(label_encoder.classes_)} классов\")\n",
        "\n",
        "    # Сохраняем пример\n",
        "    save_df = processed_df[['original_text', 'abbreviation', 'expansion']].copy()\n",
        "    save_df.to_csv(config.CSV_PATH, index=False, encoding='utf-8')\n",
        "    logger.info(f\"Создан CSV файл: {config.CSV_PATH}\")\n",
        "\n",
        "    return processed_df, label_encoder\n",
        "\n",
        "# ==================== 4. ДАТАСЕТ ДЛЯ ОБУЧЕНИЯ ====================\n",
        "\n",
        "class AbbreviationDataset(Dataset):\n",
        "    \"\"\"Датасет для обучения модели распознавания аббревиатур\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        texts: List[str],\n",
        "        labels: List[int],\n",
        "        tokenizer: BertTokenizer,\n",
        "        max_length: int = 128\n",
        "    ):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:\n",
        "        text = self.texts[idx]\n",
        "\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "        }\n",
        "# ==================== 5. МОДЕЛЬ И МЕТРИКИ ====================\n",
        "\n",
        "def compute_metrics(p):\n",
        "    \"\"\"Вычисление метрик качества\"\"\"\n",
        "    predictions, labels = p\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "\n",
        "    try:\n",
        "        accuracy_metric = evaluate.load(\"accuracy\")\n",
        "        f1_metric = evaluate.load(\"f1\")\n",
        "\n",
        "        accuracy = accuracy_metric.compute(predictions=predictions, references=labels)['accuracy']\n",
        "        f1 = f1_metric.compute(predictions=predictions, references=labels, average='weighted')['f1']\n",
        "\n",
        "        return {\"accuracy\": accuracy, \"f1\": f1}\n",
        "    except:\n",
        "        # Простой расчет accuracy если evaluate не работает\n",
        "        accuracy = np.mean(predictions == labels)\n",
        "        return {\"accuracy\": accuracy, \"f1\": accuracy}\n",
        "\n",
        "# ==================== 6. АДАПТИВНОЕ РАЗДЕЛЕНИЕ ДАННЫХ ====================\n",
        "\n",
        "def adaptive_train_test_split(df: pd.DataFrame, label_col: str = 'label_id',\n",
        "                            test_size: float = 0.3, min_test_per_class: int = 1):\n",
        "    \"\"\"\n",
        "    Адаптивное разделение данных, которое гарантирует минимум примеров в тестовой выборке.\n",
        "\n",
        "    Args:\n",
        "        df: DataFrame с данными\n",
        "        label_col: колонка с метками\n",
        "        test_size: доля тестовой выборки\n",
        "        min_test_per_class: минимум примеров на класс в тестовой выборке\n",
        "\n",
        "    Returns:\n",
        "        train_df, val_df\n",
        "    \"\"\"\n",
        "\n",
        "    # Если данных мало, используем простое разделение\n",
        "    if len(df) < 20:\n",
        "        train_df = df.sample(frac=1-test_size, random_state=config.RANDOM_SEED)\n",
        "        val_df = df.drop(train_df.index)\n",
        "        return train_df, val_df\n",
        "\n",
        "    # Пытаемся использовать стратифицированное разделение\n",
        "    try:\n",
        "        # Рассчитываем количество примеров для теста\n",
        "        n_test_total = max(int(len(df) * test_size),\n",
        "                          len(df[label_col].unique()) * min_test_per_class)\n",
        "\n",
        "        # Гарантируем что тестовая выборка не слишком большая\n",
        "        n_test_total = min(n_test_total, len(df) - len(df[label_col].unique()))\n",
        "\n",
        "        if n_test_total >= len(df):\n",
        "            # Если данных очень мало, оставляем 1-2 примера для теста\n",
        "            n_test_total = min(2, len(df) - 1)\n",
        "\n",
        "        # Используем стратификацию если возможно\n",
        "        train_df, val_df = train_test_split(\n",
        "            df,\n",
        "            test_size=n_test_total/len(df),\n",
        "            random_state=config.RANDOM_SEED,\n",
        "            stratify=df[label_col],\n",
        "            shuffle=True\n",
        "        )\n",
        "\n",
        "        return train_df, val_df\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.warning(f\"Ошибка при стратифицированном разделении: {e}\")\n",
        "        logger.info(\"Использую простое случайное разделение\")\n",
        "\n",
        "        # Простое разделение\n",
        "        train_df = df.sample(frac=1-test_size, random_state=config.RANDOM_SEED)\n",
        "        val_df = df.drop(train_df.index)\n",
        "\n",
        "        return train_df, val_df\n",
        "\n",
        "# ==================== 7. ОСНОВНОЙ ПРОЦЕСС ОБУЧЕНИЯ ====================\n",
        "\n",
        "def train_model() -> Tuple[Any, BertTokenizer, LabelEncoder]:\n",
        "    \"\"\"Основная функция обучения модели\"\"\"\n",
        "\n",
        "    logger.info(\"=\" * 60)\n",
        "    logger.info(\"НАЧАЛО ОБУЧЕНИЯ МОДЕЛИ\")\n",
        "    logger.info(\"=\" * 60)\n",
        "\n",
        "    # Загрузка данных\n",
        "    df, label_encoder = load_and_prepare_data(config.CSV_PATH)\n",
        "\n",
        "    # Сохраняем label encoder\n",
        "    with open('label_encoder.pkl', 'wb') as f:\n",
        "        pickle.dump(label_encoder, f)\n",
        "\n",
        "    logger.info(f\"\\nВсего примеров: {len(df)}\")\n",
        "    logger.info(f\"Количество классов: {len(label_encoder.classes_)}\")\n",
        "\n",
        "    # Для очень маленьких данных - используем все для обучения\n",
        "    if len(df) < 10:\n",
        "        logger.warning(f\"Очень мало данных ({len(df)} примеров). Использую все данные для обучения.\")\n",
        "        train_df = df\n",
        "        val_df = df.iloc[:0]  # Пустая валидационная выборка\n",
        "    else:\n",
        "        # Используем адаптивное разделение\n",
        "        train_df, val_df = adaptive_train_test_split(\n",
        "            df,\n",
        "            label_col='label_id',\n",
        "            test_size=config.TEST_SIZE,\n",
        "            min_test_per_class=1\n",
        "        )\n",
        "\n",
        "    logger.info(f\"\\nОбучающая выборка: {len(train_df)} примеров\")\n",
        "    logger.info(f\"Валидационная выборка: {len(val_df)} примеров\")\n",
        "\n",
        "    # Проверяем распределение\n",
        "    logger.info(\"\\nРаспределение классов:\")\n",
        "    for exp in label_encoder.classes_:\n",
        "        train_count = (train_df['expansion'] == exp).sum()\n",
        "        logger.info(f\"  {exp[:30]:30} : {train_count:2}\")\n",
        "\n",
        "    # Загрузка модели и токенайзера\n",
        "    logger.info(f\"\\nЗагрузка модели: {config.MODEL_NAME}\")\n",
        "    tokenizer = BertTokenizer.from_pretrained(config.MODEL_NAME)\n",
        "\n",
        "    # Создаем датасеты\n",
        "    train_dataset = AbbreviationDataset(\n",
        "        texts=train_df['marked_text'].tolist(),\n",
        "        labels=train_df['label_id'].tolist(),\n",
        "        tokenizer=tokenizer,\n",
        "        max_length=config.MAX_LENGTH\n",
        "    )\n",
        "\n",
        "    if len(val_df) > 0:\n",
        "        val_dataset = AbbreviationDataset(\n",
        "            texts=val_df['marked_text'].tolist(),\n",
        "            labels=val_df['label_id'].tolist(),\n",
        "            tokenizer=tokenizer,\n",
        "            max_length=config.MAX_LENGTH\n",
        "        )\n",
        "    else:\n",
        "        val_dataset = None\n",
        "\n",
        "    # Загружаем модель ПОСЛЕ создания датасетов\n",
        "    # Это важно для правильной инициализации эмбеддингов\n",
        "    model = BertForSequenceClassification.from_pretrained(\n",
        "        config.MODEL_NAME,\n",
        "        num_labels=len(label_encoder.classes_),\n",
        "        ignore_mismatched_sizes=True\n",
        "    )\n",
        "\n",
        "    # НЕ добавляем специальные токены к эмбеддингам модели\n",
        "    # Это вызывает ошибку \"index out of range\"\n",
        "\n",
        "    # Адаптивные параметры обучения\n",
        "    n_train = len(train_df)\n",
        "    n_val = len(val_df) if val_df is not None else 0\n",
        "\n",
        "    effective_epochs = min(config.EPOCHS, max(3, 50 // max(1, n_train // 2)))\n",
        "    batch_size = min(config.BATCH_SIZE, max(2, n_train))\n",
        "\n",
        "    logger.info(f\"\\nАдаптивные параметры обучения:\")\n",
        "    logger.info(f\"  Количество эпох: {effective_epochs}\")\n",
        "    logger.info(f\"  Размер батча: {batch_size}\")\n",
        "    logger.info(f\"  Всего шагов: {max(1, n_train // batch_size * effective_epochs)}\")\n",
        "\n",
        "    # Определяем стратегию оценки\n",
        "    eval_strategy = \"epoch\" if n_val > 0 else \"no\"\n",
        "\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=config.OUTPUT_DIR,\n",
        "        overwrite_output_dir=True,\n",
        "        num_train_epochs=effective_epochs,\n",
        "        per_device_train_batch_size=batch_size,\n",
        "        per_device_eval_batch_size=batch_size if n_val > 0 else batch_size,\n",
        "        warmup_steps=max(1, min(10, n_train // 5)),\n",
        "        weight_decay=0.0,  # Убираем регуляризацию для маленьких данных\n",
        "        logging_dir='./logs',\n",
        "        logging_steps=max(1, min(5, n_train // batch_size)),\n",
        "        eval_strategy=eval_strategy,\n",
        "        save_strategy=\"epoch\",\n",
        "        load_best_model_at_end=n_val > 0,\n",
        "        metric_for_best_model=\"accuracy\" if n_val > 0 else None,\n",
        "        greater_is_better=True,\n",
        "        report_to=\"none\",\n",
        "        save_total_limit=1,\n",
        "        seed=config.RANDOM_SEED,\n",
        "        dataloader_num_workers=0,\n",
        "        fp16=False,\n",
        "        gradient_accumulation_steps=1,  # Убираем accumulation steps\n",
        "        learning_rate=2e-5,  # Добавляем явное указание learning rate\n",
        "    )\n",
        "\n",
        "    # Создаем тренер\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=val_dataset if n_val > 0 else None,\n",
        "        compute_metrics=compute_metrics if n_val > 0 else None,\n",
        "    )\n",
        "\n",
        "    logger.info(\"\\n\" + \"=\" * 60)\n",
        "    logger.info(\"ЗАПУСК ОБУЧЕНИЯ\")\n",
        "    logger.info(\"=\" * 60)\n",
        "\n",
        "    try:\n",
        "        train_result = trainer.train()\n",
        "\n",
        "        # Сохраняем модель\n",
        "        trainer.save_model(config.OUTPUT_DIR)\n",
        "        tokenizer.save_pretrained(config.OUTPUT_DIR)\n",
        "\n",
        "        logger.info(f\"\\nМодель сохранена в: {config.OUTPUT_DIR}\")\n",
        "\n",
        "        # Оценка если есть валидационные данные\n",
        "        if n_val > 0:\n",
        "            logger.info(\"\\n\" + \"=\" * 60)\n",
        "            logger.info(\"ФИНАЛЬНАЯ ОЦЕНКА\")\n",
        "            logger.info(\"=\" * 60)\n",
        "\n",
        "            try:\n",
        "                eval_results = trainer.evaluate()\n",
        "                for key, value in eval_results.items():\n",
        "                    if isinstance(value, float):\n",
        "                        logger.info(f\"{key:20}: {value:.4f}\")\n",
        "            except Exception as e:\n",
        "                logger.warning(f\"Не удалось вычислить метрики: {e}\")\n",
        "\n",
        "        return trainer, tokenizer, label_encoder\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Ошибка при обучении: {e}\")\n",
        "        logger.info(\"Пробую обучить с минимальными параметрами...\")\n",
        "\n",
        "        # Попробуем минимальный вариант\n",
        "        return train_minimal_model(df, label_encoder)\n",
        "\n",
        "def train_simple_model(df: pd.DataFrame, label_encoder: LabelEncoder):\n",
        "    \"\"\"Простая версия обучения для маленьких данных\"\"\"\n",
        "\n",
        "    logger.info(\"\\n\" + \"=\" * 60)\n",
        "    logger.info(\"ПРОСТОЕ ОБУЧЕНИЕ (мало данных)\")\n",
        "    logger.info(\"=\" * 60)\n",
        "\n",
        "    # Используем все данные для обучения\n",
        "    tokenizer = BertTokenizer.from_pretrained(config.MODEL_NAME)\n",
        "\n",
        "    model = BertForSequenceClassification.from_pretrained(\n",
        "        config.MODEL_NAME,\n",
        "        num_labels=len(label_encoder.classes_),\n",
        "        ignore_mismatched_sizes=True\n",
        "    )\n",
        "\n",
        "    model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "    # Создаем датасет из всех данных\n",
        "    dataset = AbbreviationDataset(\n",
        "        texts=df['marked_text'].tolist(),\n",
        "        labels=df['label_id'].tolist(),\n",
        "        tokenizer=tokenizer,\n",
        "        max_length=config.MAX_LENGTH\n",
        "    )\n",
        "\n",
        "    # Очень простые параметры\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=config.OUTPUT_DIR,\n",
        "        overwrite_output_dir=True,\n",
        "        num_train_epochs=3,\n",
        "        per_device_train_batch_size=2,\n",
        "        per_device_eval_batch_size=2,\n",
        "        warmup_steps=1,\n",
        "        weight_decay=0.0,\n",
        "        logging_dir='./logs',\n",
        "        logging_steps=1,\n",
        "        eval_strategy=\"no\",\n",
        "        save_strategy=\"epoch\",\n",
        "        report_to=\"none\",\n",
        "        seed=config.RANDOM_SEED,\n",
        "        dataloader_num_workers=0,\n",
        "        fp16=False,\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=dataset,\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        trainer.train()\n",
        "        trainer.save_model(config.OUTPUT_DIR)\n",
        "        tokenizer.save_pretrained(config.OUTPUT_DIR)\n",
        "\n",
        "        logger.info(f\"\\nМодель сохранена в: {config.OUTPUT_DIR}\")\n",
        "\n",
        "        return trainer, tokenizer, label_encoder\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Ошибка при простом обучении: {e}\")\n",
        "        raise\n",
        "\n",
        "# ==================== 8. КЛАСС ДЛЯ ПРЕДСКАЗАНИЙ ====================\n",
        "\n",
        "class AbbreviationPredictor:\n",
        "    \"\"\"Класс для предсказания расшифровок аббревиатур\"\"\"\n",
        "\n",
        "    def __init__(self, model_dir: str = None):\n",
        "        if model_dir is None:\n",
        "            model_dir = config.OUTPUT_DIR\n",
        "\n",
        "        if not os.path.exists(model_dir):\n",
        "            raise FileNotFoundError(f\"Директория модели не найдена: {model_dir}\")\n",
        "\n",
        "        self.tokenizer = BertTokenizer.from_pretrained(model_dir)\n",
        "\n",
        "        special_tokens_dict = {\n",
        "            'additional_special_tokens': list(config.SPECIAL_TOKENS.values())\n",
        "        }\n",
        "        self.tokenizer.add_special_tokens(special_tokens_dict)\n",
        "\n",
        "        self.model = BertForSequenceClassification.from_pretrained(model_dir)\n",
        "        self.model.eval()\n",
        "        self.model.resize_token_embeddings(len(self.tokenizer))\n",
        "\n",
        "        # Загрузка label encoder\n",
        "        encoder_path = os.path.join(model_dir, 'label_encoder.pkl')\n",
        "        if os.path.exists(encoder_path):\n",
        "            with open(encoder_path, 'rb') as f:\n",
        "                self.label_encoder = pickle.load(f)\n",
        "        elif os.path.exists('label_encoder.pkl'):\n",
        "            with open('label_encoder.pkl', 'rb') as f:\n",
        "                self.label_encoder = pickle.load(f)\n",
        "        else:\n",
        "            # Создаем простой encoder если не найден\n",
        "            logger.warning(\"Label encoder не найден. Создаю новый...\")\n",
        "            self.label_encoder = LabelEncoder()\n",
        "            # Нужно будет дообучить или использовать предопределенные классы\n",
        "\n",
        "        self.classes = getattr(self.label_encoder, 'classes_', [])\n",
        "        logger.info(f\"Модель загружена. Количество классов: {len(self.classes)}\")\n",
        "\n",
        "    def predict_single(\n",
        "        self,\n",
        "        text: str,\n",
        "        abbreviation: str,\n",
        "        position: Optional[int] = None,\n",
        "        top_k: int = 3\n",
        "    ) -> Dict[str, Any]:\n",
        "        \"\"\"Предсказание расшифровки для одной аббревиатуры в тексте.\"\"\"\n",
        "\n",
        "        # Маркируем аббревиатуру\n",
        "        marked_text, abbr_start = AbbreviationProcessor.mark_specific_abbreviation(\n",
        "            text, abbreviation, position\n",
        "        )\n",
        "\n",
        "        # Токенизация\n",
        "        inputs = self.tokenizer(\n",
        "            marked_text,\n",
        "            return_tensors=\"pt\",\n",
        "            max_length=config.MAX_LENGTH,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True\n",
        "        )\n",
        "\n",
        "        # Предсказание\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(**inputs)\n",
        "            probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)[0]\n",
        "\n",
        "        # Получаем топ-K предсказаний\n",
        "        top_k = min(top_k, len(self.classes))\n",
        "        top_probs, top_indices = torch.topk(probabilities, top_k)\n",
        "\n",
        "        predictions = []\n",
        "        for prob, idx in zip(top_probs, top_indices):\n",
        "            if len(self.classes) > 0:\n",
        "                expansion = self.label_encoder.inverse_transform([idx.cpu().numpy()])[0]\n",
        "            else:\n",
        "                expansion = f\"Class_{idx.item()}\"\n",
        "\n",
        "            predictions.append({\n",
        "                'expansion': expansion,\n",
        "                'confidence': prob.item(),\n",
        "                'label_id': idx.item()\n",
        "            })\n",
        "\n",
        "        return {\n",
        "            'text': text,\n",
        "            'abbreviation': abbreviation,\n",
        "            'prediction': predictions[0]['expansion'],\n",
        "            'confidence': predictions[0]['confidence'],\n",
        "            'all_predictions': predictions,\n",
        "        }\n",
        "\n",
        "# ==================== 9. ТЕСТИРОВАНИЕ ====================\n",
        "\n",
        "def run_simple_tests(predictor: AbbreviationPredictor):\n",
        "    \"\"\"Простое тестирование модели\"\"\"\n",
        "\n",
        "    logger.info(\"\\n\" + \"=\" * 60)\n",
        "    logger.info(\"ТЕСТИРОВАНИЕ МОДЕЛИ\")\n",
        "    logger.info(\"=\" * 60)\n",
        "\n",
        "    test_cases = [\n",
        "        (\"Используйте API для доступа к данным.\", \"API\"),\n",
        "        (\"Процессор CPU работает быстро.\", \"CPU\"),\n",
        "        (\"Сайт использует HTTP протокол.\", \"HTTP\"),\n",
        "        (\"Подключитесь через VPN.\", \"VPN\"),\n",
        "    ]\n",
        "\n",
        "    for text, abbr in test_cases:\n",
        "        logger.info(f\"\\nТекст: {text}\")\n",
        "        logger.info(f\"Аббревиатура: {abbr}\")\n",
        "\n",
        "        try:\n",
        "            result = predictor.predict_single(text, abbr)\n",
        "            logger.info(f\"Предсказание: {result['prediction']}\")\n",
        "            logger.info(f\"Уверенность: {result['confidence']:.2%}\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Ошибка: {e}\")\n",
        "\n",
        "# ==================== 10. ОСНОВНОЙ БЛОК ====================\n",
        "\n",
        "def main():\n",
        "    \"\"\"Основная функция\"\"\"\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"СИСТЕМА РАСШИФРОВКИ АББРЕВИАТУР\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Проверяем наличие GPU\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    logger.info(f\"Используется устройство: {device}\")\n",
        "\n",
        "    # Создаем CSV если его нет\n",
        "    if not os.path.exists(config.CSV_PATH):\n",
        "        logger.info(f\"Создаю пример CSV файла: {config.CSV_PATH}\")\n",
        "        create_balanced_sample_dataset()\n",
        "\n",
        "    try:\n",
        "        # Обучаем модель\n",
        "        trainer, tokenizer, label_encoder = train_model()\n",
        "\n",
        "        # Создаем предсказатель\n",
        "        predictor = AbbreviationPredictor(config.OUTPUT_DIR)\n",
        "\n",
        "        # Тестируем\n",
        "        run_simple_tests(predictor)\n",
        "\n",
        "        print(\"\\n\" + \"=\" * 60)\n",
        "        print(\"ОБУЧЕНИЕ ЗАВЕРШЕНО УСПЕШНО!\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        # Пример использования\n",
        "        print(\"\\nПример использования:\")\n",
        "        print(\"```python\")\n",
        "        print(\"from your_module import AbbreviationPredictor\")\n",
        "        print()\n",
        "        print(\"predictor = AbbreviationPredictor()\")\n",
        "        print('result = predictor.predict_single(\"Используйте API для доступа\", \"API\")')\n",
        "        print('print(f\"Расшифровка: {result[\\'prediction\\']}\")')\n",
        "        print(\"```\")\n",
        "\n",
        "        return predictor\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Критическая ошибка: {e}\")\n",
        "        print(\"\\n\" + \"=\" * 60)\n",
        "        print(\"ВОЗНИКЛИ ПРОБЛЕМЫ\")\n",
        "        print(\"=\" * 60)\n",
        "        print(\"\\nРекомендации:\")\n",
        "        print(\"1. Убедитесь что файл abbreviations_dataset.csv существует\")\n",
        "        print(\"2. Проверьте что в файле есть колонки: text, abbreviation, expansion\")\n",
        "        print(\"3. Добавьте больше данных (минимум 2 примера на класс)\")\n",
        "        print(\"4. Убедитесь что установлены все зависимости\")\n",
        "\n",
        "        return None\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    predictor = main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 777,
          "referenced_widgets": [
            "c42bbc3f77be45009c021f37b55469f2",
            "2cc7c759dce44826bb2deae9ba6e4291",
            "4f24e19829704d118bbba90ba4a9d9dd",
            "c3bf29ec555e49ecbb344706c5d0219e",
            "d31a7498916f43958a542edb3d48b04b",
            "9f81448da29c493faffb6f01653e7a57",
            "4af55b527b754eb8bcc2a0ab5405750b",
            "e7b4eb15c2664114ada1536896ed641d",
            "2d621f733d944f9d8ff0a953beef46f7",
            "3a3142c7b96f44c690ced214e47cdd3e",
            "7f4c0a9e046748b88b394516cc1dd28d",
            "0df4ee3ce7f2479f904b1741b8b5c5f5",
            "94adb047d70d42d6a27a17cec08ef45f",
            "2b30ede3492942fa9456f5f6732c7e15",
            "f4774f7c3be74ae383ce81f2fcdcf0c3",
            "5ce53bfa7f8c4c33a079491f1c2b898e",
            "3e7aaea13a364ef786afe4965df59046",
            "4faad44d87cc454390fc14eb7c5e9bab",
            "47b7af2a0e5e431d86504146b940f820",
            "79399796517d4a588063ab5bb2f5b61c",
            "0b37c138c816462d86a8c0442d28a679",
            "9523460ba94d4ee699acd72d2609c046"
          ]
        },
        "id": "--ISvol3V-Af",
        "outputId": "8b8d2e0f-de4b-44b9-b8c8-d1c7cb2ba988"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "СИСТЕМА РАСШИФРОВКИ АББРЕВИАТУР\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at cointegrated/rubert-tiny2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='32' max='32' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [32/32 00:41, Epoch 8/8]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.078200</td>\n",
              "      <td>2.096471</td>\n",
              "      <td>0.166667</td>\n",
              "      <td>0.083333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>2.050600</td>\n",
              "      <td>2.100928</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>2.012300</td>\n",
              "      <td>2.107200</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>2.040900</td>\n",
              "      <td>2.112417</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>2.034800</td>\n",
              "      <td>2.116604</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>1.965900</td>\n",
              "      <td>2.119481</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>2.002100</td>\n",
              "      <td>2.121349</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>2.014400</td>\n",
              "      <td>2.122189</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading builder script: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c42bbc3f77be45009c021f37b55469f2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading builder script: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0df4ee3ce7f2479f904b1741b8b5c5f5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2/2 00:00]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "ОБУЧЕНИЕ ЗАВЕРШЕНО УСПЕШНО!\n",
            "============================================================\n",
            "\n",
            "Пример использования:\n",
            "```python\n",
            "from your_module import AbbreviationPredictor\n",
            "\n",
            "predictor = AbbreviationPredictor()\n",
            "result = predictor.predict_single(\"Используйте API для доступа\", \"API\")\n",
            "print(f\"Расшифровка: {result['prediction']}\")\n",
            "```\n"
          ]
        }
      ]
    }
  ]
}